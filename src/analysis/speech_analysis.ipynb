{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: altair in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (5.3.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/gridsan/ktiwary/.local/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: jinja2 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair) (3.1.4)\n",
      "Requirement already satisfied: packaging in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair) (24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair) (4.12.1)\n",
      "Requirement already satisfied: toolz in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair) (0.12.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair) (4.22.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from jsonschema>=3.0->altair) (0.35.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from jsonschema>=3.0->altair) (2023.12.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from jsonschema>=3.0->altair) (23.2.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from jsonschema>=3.0->altair) (0.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from jinja2->altair) (2.1.5)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting semanticscholar\n",
      "  Downloading semanticscholar-0.8.2-py3-none-any.whl (24 kB)\n",
      "Collecting tenacity\n",
      "  Downloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nest-asyncio in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from semanticscholar) (1.6.0)\n",
      "Collecting anyio\n",
      "  Downloading anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna\n",
      "  Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sniffio\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting certifi\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.4/164.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting exceptiongroup>=1.0.2\n",
      "  Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from anyio->httpx->semanticscholar) (4.12.1)\n",
      "Installing collected packages: tenacity, sniffio, idna, h11, exceptiongroup, certifi, httpcore, anyio, httpx, semanticscholar\n",
      "Successfully installed anyio-4.4.0 certifi-2024.6.2 exceptiongroup-1.2.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 semanticscholar-0.8.2 sniffio-1.3.1 tenacity-8.3.0\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !which -a pip3\n",
    "!/home/gridsan/ktiwary/.conda/envs/dpi310/bin/pip3 install pandas altair\n",
    "!/home/gridsan/ktiwary/.conda/envs/dpi310/bin/pip3 install semanticscholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Append system path\n",
    "sys.path = [p for p in sys.path if not p.endswith(\"../..\")]  # Cleans duplicated '../..'\n",
    "sys.path.insert(0, \"../\")  # This adds `src` to the path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows() # Allow using more than 5000 rows, for now\n",
    "from collections import defaultdict\n",
    "from helpers import io, filters\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Utility functions to process and transform data summaries:\n",
    "\n",
    "---\n",
    "```python\n",
    "def invert_dict_of_lists(\n",
    "  d: dict[str, list[str]]\n",
    ") -> dict[str, str]\n",
    "```\n",
    "- Inverts a dictionary of lists for easier mapping of constants.\n",
    "---\n",
    "```python\n",
    "def remap_licenses_with_paraphrases(\n",
    "  summaries: list[dict[str, Any]],\n",
    "  paraphrases: dict[str, str]\n",
    ") -> dict[str, Any]\n",
    "``` \n",
    "- Standardizes inconsistent license names in data summaries using predefined paraphrases.\n",
    "---\n",
    "```python\n",
    "def map_license_criteria_multimodal(\n",
    "  data_summary: list[dict[str, Any]],\n",
    "  all_constants: dict[str, dict[str, list[str]]]\n",
    ") -> list[dict[str, Any]]\n",
    "```\n",
    "- Maps license criteria for multimodal datasets, resolving them according to predefined constants.\n",
    "---\n",
    "```python\n",
    "def get_country(x: str) -> list[int]\n",
    "```\n",
    "- Takes a country name as input and returns a list of ISO3166 codes (mostly, of length 1). It handles a special case that appears in some text annotations (\"African Continent\" -> list of ISO codes) and logs a warning for any countries not found in the mapping.\n",
    "---\n",
    "```python\n",
    "def gini(array: np.ndarray) -> float:\n",
    "```\n",
    "- Takes an array of values and computes the Gini coefficient.\n",
    "---\n",
    "```python\n",
    "def factor_year(\n",
    "  df: pd.DataFrame,\n",
    "  column: str = \"Year Released\",\n",
    "  min_year: int = 2013\n",
    ") -> pd.DataFrame:\n",
    "```\n",
    "- Converts the year column into a categorical variable (with years before a given value grouped together).\n",
    "---\n",
    "```python\n",
    "def order_by_grouped_permisiveness(\n",
    "        df: pd.DataFrame,\n",
    "        group_column: str,\n",
    "        licensetype_column: str = \"License Type\",\n",
    "        permissive_licensetypes: list[str] = [\"Commercial\"]\n",
    ") -> pd.Series:\n",
    "```\n",
    "- Computes permisiveness (proportion of license types in a given set, by default only those marked `Commercial`) by a given grouping factor and returns an order for that factor.\n",
    "---\n",
    "```python\n",
    "def reduce_categories_to_topk(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    k: int = 6\n",
    ") -> pd.DataFrame:\n",
    "```\n",
    "- Reduces the number of categories in a column to `k`, with the rest grouped under `Other`. So returns a `DataFrame` with a version of that column with `k + 1` total categories.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_dict_of_lists(d: dict[str, list[str]]) -> dict[str, str]:\n",
    "    \"\"\"Useful for mapping constants, paraphrases, etc.\n",
    "    These are normally in the form:\n",
    "        { \"Category\": [\"item1\", \"item2\", … ] }\n",
    "    Whereas we want to invert it to:\n",
    "        { \"item1\": \"Category\", \"item2\": \"Category\", … }\n",
    "    \"\"\"\n",
    "    inverted = {}\n",
    "    for k, v in d.items():\n",
    "        for item in v:\n",
    "            inverted[item] = k\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_licenses_with_paraphrases(\n",
    "        summaries: list[dict[str, Any]],\n",
    "        paraphrases: dict[str, str]\n",
    "    ) -> dict[str, Any]:\n",
    "    \"\"\"Map inconsistent license names to shared paraphrases using the constants.\n",
    "    E.g. \"CC-BY-SA 4.0\", \"CC BY SA 4.0\" -> \"CC BY-SA 4.0\"\n",
    "    \"\"\"\n",
    "\n",
    "    for i, summary in enumerate(summaries):\n",
    "        for j, license in enumerate(summary[\"Licenses\"]):\n",
    "            license = license[\"License\"]\n",
    "            summaries[i][\"Licenses\"][j][\"License\"] = paraphrases.get(\n",
    "                license,\n",
    "                license\n",
    "            )\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_resolve_licenses(\n",
    "    license_infos: list[tuple[str, str]],\n",
    "    all_constants: dict[str, dict[str, list[str]]]\n",
    ") -> list[str]:\n",
    "    \"\"\"Function taken from `text_ft_plots.ipynb`\"\"\"\n",
    "    classified_licenses = []\n",
    "    for (license_name, license_url) in license_infos:\n",
    "        # Classify an individual license\n",
    "        classifications = filters.classify_license(license_name, license_url, all_constants)\n",
    "        classified_licenses.append(classifications)\n",
    "\n",
    "    # By default, multiple licenses yield to the most restrictive one\n",
    "    resolved_criteria = filters.resolve_multiple_licenses(classified_licenses)\n",
    "    return resolved_criteria\n",
    "\n",
    "\n",
    "def add_license_classes_to_summaries(\n",
    "    data_summary: list[dict[str, Any]],\n",
    "    resolved_classes: dict[str, list[str]],\n",
    "    aggregator: str\n",
    "):\n",
    "    \"\"\"Function taken from `text_ft_plots.ipynb`\"\"\"\n",
    "    # Update DataFrame with columns for use, attribution, share_alike\n",
    "    for row in data_summary:\n",
    "        row[f\"License Use ({aggregator})\"] = resolved_classes[row[\"Unique Dataset Identifier\"]][0]\n",
    "        row[f\"License Attribution ({aggregator})\"] = resolved_classes[row[\"Unique Dataset Identifier\"]][1]\n",
    "        row[f\"License Share Alike ({aggregator})\"] = resolved_classes[row[\"Unique Dataset Identifier\"]][2]\n",
    "    return data_summary\n",
    "\n",
    "\n",
    "def map_license_criteria_multimodal(\n",
    "    data_summary: list[dict[str, Any]],\n",
    "    all_constants: dict[str, dict[str, list[str]]]\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Variant of `map_license_criteria` that works with multimodal datasets.\n",
    "    Simplified to only include `Licenses` (not HF, etc.).\n",
    "\n",
    "    Function adapted from `text_ft_plots.ipynb`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack licenses for each dataset. {uid --> (license_name, license_url)}\n",
    "    our_uid_to_license_infos = defaultdict(list)\n",
    "\n",
    "    # Same as ours, but excludes OpenAI Terms:\n",
    "    our_uid_to_license_infos_no_openai = defaultdict(list)\n",
    "\n",
    "    for row in data_summary:\n",
    "        uid = row[\"Unique Dataset Identifier\"]\n",
    "        for license_info in row[\"Licenses\"]:\n",
    "            license_name = license_info[\"License\"]\n",
    "            license_url = license_info.get(\"License URL\", None) # FOR NOW\n",
    "            our_uid_to_license_infos[uid].append((license_name, license_url))\n",
    "            if license_info[\"License\"] != \"OpenAI\":\n",
    "                our_uid_to_license_infos_no_openai[uid].append((license_name, license_url))\n",
    "\n",
    "        # If OpenAI was the only license, we add Unspecified so there isn't nothing there.\n",
    "        if len(our_uid_to_license_infos_no_openai[uid]) == 0:\n",
    "            our_uid_to_license_infos_no_openai[uid].append((\"Unspecified\", None))\n",
    "\n",
    "\n",
    "    # classify and resolve licenses for each dataset and each aggregator\n",
    "    ours_resolved, ours_openai_resolved = {}, {}\n",
    "    for uid in our_uid_to_license_infos.keys():\n",
    "        ours_resolved[uid] = classify_and_resolve_licenses(our_uid_to_license_infos[uid], all_constants)\n",
    "        ours_openai_resolved[uid] = classify_and_resolve_licenses(our_uid_to_license_infos_no_openai[uid], all_constants)\n",
    "\n",
    "\n",
    "    data_summary = add_license_classes_to_summaries(data_summary, ours_resolved, \"DataProvenance\")\n",
    "    data_summary = add_license_classes_to_summaries(data_summary, ours_openai_resolved, \"DataProvenance IgnoreOpenAI\")\n",
    "\n",
    "    return data_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(array: np.ndarray) -> float:\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array.\n",
    "\n",
    "    Implementation taken from: https://github.com/oliviaguest/gini\n",
    "    \"\"\"\n",
    "    # based on bottom eq:\n",
    "    # http://www.statsdirect.com/help/generatedimages/equations/equation154.svg\n",
    "    # from:\n",
    "    # http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "    # All values are treated equally, arrays must be 1d:\n",
    "    array = array.flatten()\n",
    "    if np.amin(array) < 0:\n",
    "        # Values cannot be negative:\n",
    "        array -= np.amin(array)\n",
    "    # Values cannot be 0:\n",
    "    array = array + 0.0000001\n",
    "    # Values must be sorted:\n",
    "    array = np.sort(array)\n",
    "    # Index per array element:\n",
    "    index = np.arange(1,array.shape[0]+1)\n",
    "    # Number of array elements:\n",
    "    n = array.shape[0]\n",
    "    # Gini coefficient:\n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_year(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = \"Year Released\",\n",
    "    min_year: int = 2013\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Transform the year column into a categorical column.\n",
    "\n",
    "    Years before `min_year` are grouped into a category, i.e. \"<`min_year`\" (e.g. )\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    min_yeartext = \"<%d\" % min_year\n",
    "    max_year = df[column].max()\n",
    "\n",
    "    df[column] = df[column].map(\n",
    "        lambda x: min_yeartext if (x < min_year) else str(x)\n",
    "    )\n",
    "\n",
    "    order = [min_yeartext, *map(str, range(min_year, max_year + 1))]\n",
    "\n",
    "    df[column] = pd.Categorical(\n",
    "        df[column],\n",
    "        categories=order,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    return df, order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_by_grouped_permisiveness(\n",
    "        df: pd.DataFrame,\n",
    "        group_column: str,\n",
    "        licensetype_column: str = \"License Type\",\n",
    "        permissive_licensetypes: list[str] = [\"Commercial\"]\n",
    ") -> pd.Series:\n",
    "    \"\"\"Given a DataFrame, group it by `group_column` and calculate the permissiveness of each group.\n",
    "\n",
    "    Permisiveness is calculated as the proportion of licenses that are in `permissive_licensetypes`.\n",
    "    \"\"\"\n",
    "    permisiveness = df.groupby(group_column).apply(\n",
    "        lambda x: (x[licensetype_column].isin(permissive_licensetypes)).mean()\n",
    "    ).reset_index(name=\"Permissiveness\")\n",
    "\n",
    "    permisiveness_order = permisiveness.sort_values(by=\"Permissiveness\")[group_column].tolist()\n",
    "\n",
    "    return permisiveness_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_categories_to_topk(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    k: int = 6\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Reduce the number of categories in a column to the top `k` categories.\n",
    "\n",
    "    The rest are grouped into an \"Other\" category.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    topk = df[column].value_counts().head(k).index.tolist()\n",
    "    df[column] = df[column].map(\n",
    "        lambda x: x if x in topk else \"Other\"\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Constants and Summaries\n",
    "\n",
    "Load constants and data summaries from JSON files. Constants provide mappings and criteria for licenses, creator groups, various other categories. Data summaries contain modality-specific information about datasets.\n",
    "\n",
    "- `all_constants`: Dictionary containing all predefined constants.\n",
    "- `speech_summaries`: Data summaries for speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_constants = io.read_all_constants(\"../../constants/\")\n",
    "# speech_summaries = io.read_data_summary_json(\"../../data_summaries-speech/\")\n",
    "speech_summaries = io.read_data_summary_json(\"/home/gridsan/ktiwary/src/dpi-ktiwary-fork/data_summaries/video/test-0520\")\n",
    "\n",
    "# license_paraphrases = invert_dict_of_lists(all_constants[\"LICENSE_PARAPHRASES\"])\n",
    "license_paraphrases = invert_dict_of_lists(all_constants[\"LICENSE_CLASSES\"])\n",
    "speech_summaries = map_license_criteria_multimodal(\n",
    "    remap_licenses_with_paraphrases(\n",
    "        speech_summaries,\n",
    "        license_paraphrases\n",
    "    ),\n",
    "    all_constants\n",
    ")\n",
    "\n",
    "df_speech = pd.DataFrame(speech_summaries)\n",
    "df_speech, YEARS_ORDER = factor_year(df_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9426121290947573"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall Gini coefficient (hours by dataset)\n",
    "gini(df_speech[\"Video Hours\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting constants\n",
    "FONT_SIZE = 16\n",
    "LEGEND_POSITION = \"bottom\"\n",
    "PLOT_TOFILE = True # Whether and where to output plots\n",
    "# PLOT_DIR = \"~/Dropbox (MIT)/dpi-plotsspeech/\"\n",
    "PLOT_DIR = \"/home/gridsan/ktiwary/src/dpi-ktiwary-fork/dpi-plots\"\n",
    "PLOT_PPI = 300\n",
    "MAX_LABELLIMIT = 1000 # Large number to avoid label summarization in plots\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    PLOT_DIR = os.path.expanduser(PLOT_DIR)\n",
    "    os.makedirs(PLOT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License Use by Language Family and Source Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting constants\n",
    "LICENSE_ORDER = [\"Non-Commercial/Academic\", \"Unspecified\", \"Commercial\"]\n",
    "LICENSE_PALETTE = [\"#e04c71\", \"#e0cd92\", \"#82b5cf\"]\n",
    "LICENSE_PLOTW = 600\n",
    "LICENSE_PLOTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to main DPI license types\n",
    "df_speech[\"License Type\"] = df_speech[\"License Use (DataProvenance)\"].map({\n",
    "    \"academic-only\": \"Non-Commercial/Academic\",\n",
    "    \"non-commercial\": \"Non-Commercial/Academic\",\n",
    "    \"unspecified\": \"Unspecified\",\n",
    "    \"commercial\": \"Commercial\"\n",
    "})\n",
    "\n",
    "df_speech[\"License Type\"] = pd.Categorical(\n",
    "    df_speech[\"License Type\"],\n",
    "    categories=LICENSE_ORDER,\n",
    "    ordered=True\n",
    ")\n",
    "df_speech = df_speech.sort_values(by=\"License Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unique Dataset Identifier', 'Collection', 'Collection URL',\n",
       "       'Dataset Name', 'Paper Title', 'Paper URL', 'GitHub URL',\n",
       "       'Hugging Face URL', 'Papers with Code URL', 'ArXiv URL',\n",
       "       'Semantic Scholar Corpus ID', 'Year Released', 'Text Sources',\n",
       "       'Video Task', 'Licenses', 'Creators', 'Countries',\n",
       "       'License Verified By', 'Video Hours', 'Taken Down', 'Video Sources',\n",
       "       'License Use (DataProvenance)', 'License Attribution (DataProvenance)',\n",
       "       'License Share Alike (DataProvenance)',\n",
       "       'License Use (DataProvenance IgnoreOpenAI)',\n",
       "       'License Attribution (DataProvenance IgnoreOpenAI)',\n",
       "       'License Share Alike (DataProvenance IgnoreOpenAI)', 'License Type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remap language families for condensed plots\n",
    "df_speechlanguages = df_speech.explode(\"Video Hours\")\n",
    "df_speechlanguages[\"Video Hours\"] = df_speechlanguages[\"Video Hours\"].astype(float)\n",
    "\n",
    "df_speechlanguages.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-c2afee925e714e549b9a5fe38b143c9b.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-c2afee925e714e549b9a5fe38b143c9b.vega-embed details,\n",
       "  #altair-viz-c2afee925e714e549b9a5fe38b143c9b.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-c2afee925e714e549b9a5fe38b143c9b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-c2afee925e714e549b9a5fe38b143c9b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-c2afee925e714e549b9a5fe38b143c9b\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 16, \"titleFontSize\": 16}, \"legend\": {\"labelFontSize\": 16, \"labelLimit\": 1000, \"orient\": \"bottom\", \"titleFontSize\": 16}}, \"layer\": [{\"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"License Type\", \"scale\": {\"domain\": [\"Non-Commercial/Academic\", \"Unspecified\", \"Commercial\"], \"range\": [\"#e04c71\", \"#e0cd92\", \"#82b5cf\"]}, \"title\": \"License Type\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30}, \"field\": \"Video Task\", \"sort\": null, \"title\": \"Video Task\", \"type\": \"nominal\"}, \"y\": {\"aggregate\": \"count\", \"axis\": {\"format\": \"%\"}, \"stack\": \"normalize\", \"title\": \"Pct. Datasets\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"top\", \"dy\": -68, \"fontSize\": 12}, \"encoding\": {\"text\": {\"aggregate\": \"count\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"Video Task\", \"sort\": null, \"title\": \"Video Task to License Type\", \"type\": \"nominal\"}}}], \"data\": {\"name\": \"data-d49c4db75e1ab114c780b00069391571\"}, \"height\": 100, \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-d49c4db75e1ab114c780b00069391571\": [{\"Unique Dataset Identifier\": \"100doh\", \"Collection\": \"100doh\", \"Collection URL\": \"https://arxiv.org/abs/2006.06669\", \"Dataset Name\": \"100DOH: Understanding Human Hands in Contact at Internet Scale (CVPR 2020)\\n\", \"Paper Title\": \"100DOH: Understanding Human Hands in Contact at Internet Scale (CVPR 2020)\\n\", \"Paper URL\": \"https://arxiv.org/abs/2006.06669\", \"GitHub URL\": \"https://github.com/ddshan/hand_object_detector\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/understanding-human-hands-in-contact-at-1\", \"ArXiv URL\": \"https://arxiv.org/abs/2006.06669\", \"Semantic Scholar Corpus ID\": 215413188, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Egocentric View\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/download.html\"}], \"Creators\": [\"University of Michigan\", \"Johns Hopkins University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 4577.3, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"voxceleb\", \"Collection\": \"voxceleb\", \"Collection URL\": \"https://arxiv.org/abs/1706.08612\", \"Dataset Name\": \"VoxCeleb\", \"Paper Title\": \"VoxCeleb\", \"Paper URL\": \"https://arxiv.org/abs/1706.08612\", \"GitHub URL\": \"https://github.com/a-nagrani/VGGVox\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://cs.paperswithcode.com/paper/voxceleb-a-large-scale-speaker-identification\", \"ArXiv URL\": \"https://arxiv.org/abs/1706.08612\", \"Semantic Scholar Corpus ID\": 10475843, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Classification\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\", \"South Korea\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"finegym-dataset\", \"Collection\": \"finegym-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2004.06704\", \"Dataset Name\": \"FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding (CVPR 2020)\\n\", \"Paper Title\": \"FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding (CVPR 2020)\\n\", \"Paper URL\": \"https://arxiv.org/abs/2004.06704\", \"GitHub URL\": \"https://github.com/SDOlivia/FineGym/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/finegym\", \"ArXiv URL\": \"https://arxiv.org/abs/2004.06704\", \"Semantic Scholar Corpus ID\": 215754360, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://sdolivia.github.io/FineGym/\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 708.0, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"coin-dataset\", \"Collection\": \"coin-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1903.02874\", \"Dataset Name\": \"COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019)\", \"Paper Title\": \"COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019)\", \"Paper URL\": \"https://arxiv.org/abs/1903.02874\", \"GitHub URL\": \"https://github.com/coin-dataset/annotations\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/coin#:~:text=The%20COIN%20dataset%20(a%20large,are%20all%20collected%20from%20YouTube.\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.02874\", \"Semantic Scholar Corpus ID\": 71147568, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Classification\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/coin-dataset/annotations?tab=readme-ov-file#license\"}], \"Creators\": [\"Tsinghua University\", \"Meitu Inc.\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 476.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"trecvid\", \"Collection\": \"trecvid\", \"Collection URL\": \"https://arxiv.org/abs/2009.09984\", \"Dataset Name\": \"TRECVID\", \"Paper Title\": \"TRECVID\", \"Paper URL\": \"https://arxiv.org/abs/2009.09984\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/trecvid-2019-an-evaluation-campaign-to/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.09984\", \"Semantic Scholar Corpus ID\": 212694843, \"Year Released\": \"2019\", \"Text Sources\": [\"undisclosed web\", \"bbc\"], \"Video Task\": \"Video and Language, Video Captioning, Video Classification\", \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://trecvid.nist.gov/\"}], \"Creators\": [\"National Institute of Standards and Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\", \"bbc\"], \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"videolt-dataset\", \"Collection\": \"videolt-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2105.02668\", \"Dataset Name\": \"VideoLT: Large-scale Long-tailed Video Recognition\\n\", \"Paper Title\": \"VideoLT: Large-scale Long-tailed Video Recognition\\n\", \"Paper URL\": \"https://arxiv.org/abs/2105.02668\", \"GitHub URL\": \"https://github.com/17Skye17/VideoLT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/videolt\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.02668\", \"Semantic Scholar Corpus ID\": 233864776, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Classification\", \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://github.com/17Skye17/VideoLT?tab=readme-ov-file#data-preparation\"}], \"Creators\": [\"Fudan University\", \"Shanghai Collaborative Innovation Center of Intelligent Visual Computing\", \"Inception Institute of Artificial Intelligence\", \"University of Maryland\"], \"Countries\": [\"China\", \"UAE\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13664.96, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"vlog-vids\", \"Collection\": \"vlog-vids\", \"Collection URL\": \"https://arxiv.org/abs/1712.02310\", \"Dataset Name\": \"VLOG: From Lifestyle Vlogs to Everyday Interactions (CVPR 2018)\\n\", \"Paper Title\": \"VLOG: From Lifestyle Vlogs to Everyday Interactions (CVPR 2018)\\n\", \"Paper URL\": \"https://arxiv.org/abs/1712.02310\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/vlog-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/1712.02310\", \"Semantic Scholar Corpus ID\": 22264672, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Classification\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://web.eecs.umich.edu/~fouhey/2017/VLOG/index.html\"}], \"Creators\": [\"UC Berkeley\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 336.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"epic-kitchenes\", \"Collection\": \"epic-kitchenes\", \"Collection URL\": \"https://arxiv.org/abs/1804.02748\", \"Dataset Name\": \"EPIC-KITCHENS\", \"Paper Title\": \"EPIC-KITCHENS\", \"Paper URL\": \"https://arxiv.org/abs/1804.02748\", \"GitHub URL\": \"https://github.com/epic-kitchens\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/epic-kitchens-100\", \"ArXiv URL\": \"https://arxiv.org/abs/1804.02748\", \"Semantic Scholar Corpus ID\": 4710439, \"Year Released\": \"2018\", \"Text Sources\": [\"human\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://epic-kitchens.github.io/2024\"}], \"Creators\": [\"University of Toronto\", \"University of Bristol\", \"University of Catania\"], \"Countries\": [\"United Kingdom\", \"Canada\", \"Spain\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"thumos-challenge\", \"Collection\": \"thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Dataset Name\": \"The THUMOS Challenge on Action Recognition for\\nVideos \\u201cin the Wild\\u201d\", \"Paper Title\": \"The THUMOS Challenge on Action Recognition for\\nVideos \\u201cin the Wild\\u201d\", \"Paper URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/thumos14-1\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.06182.pdf\", \"Semantic Scholar Corpus ID\": 14049355, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLScs9davISAtYQS7SEF5qQNu0jUpLzNH3aHmPfuqk2q1VYDkmw/viewform\"}], \"Creators\": [\"Stanford University\", \"University of Central Florida\", \"Fudan University\", \"Inria\"], \"Countries\": [\"United States of America\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 254.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"50salads\", \"Collection\": \"50salads\", \"Collection URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"Dataset Name\": \"50Salads\", \"Paper Title\": \"50Salads\", \"Paper URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/50-salads\", \"ArXiv URL\": \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b9410401cec076baef045e83953f3ff24f25d149\", \"Semantic Scholar Corpus ID\": 2333743, \"Year Released\": \"2013\", \"Text Sources\": [\"human\"], \"Video Task\": \"Action Segmentation\", \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/\"}], \"Creators\": [\"University of Dundee\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 40.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"hyu-vids\", \"Collection\": \"hyu-vids\", \"Collection URL\": \"https://arxiv.org/abs/1904.11451\", \"Dataset Name\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\\n\", \"Paper Title\": \"HVU: Large Scale Holistic Video Understanding (ECCV 2020)\\n\", \"Paper URL\": \"https://arxiv.org/abs/1904.11451\", \"GitHub URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/holistic-large-scale-video-understanding\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.11451\", \"Semantic Scholar Corpus ID\": 131777079, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Classification\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/holistic-video-understanding/HVU-Dataset\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"ETH Z\\u00fcrich\", \"KU Leuven\", \"University of Bonn\", \"Sensifai\"], \"Countries\": [\"Switzerland\", \"Belgium\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 96166.67, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"toyota-smarthome\\n\", \"Collection\": \"toyota-smarthome\\n\", \"Collection URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"Dataset Name\": \"Toyota Smarthome: Real-World Activities of Daily Living (ICCV 2019)\\n\", \"Paper Title\": \"Toyota Smarthome: Real-World Activities of Daily Living (ICCV 2019)\\n\", \"Paper URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf\", \"Semantic Scholar Corpus ID\": 207971208, \"Year Released\": \"2019\", \"Text Sources\": [\"human\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://project.inria.fr/toyotasmarthome/files/2020/12/License_v2.pdf\"}], \"Creators\": [\"Toyota\"], \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 268.58, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"oops-dataset\", \"Collection\": \"oops-dataset\", \"Collection URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"Dataset Name\": \"Oops!: Predicting Unintentional Action in Video (CVPR 2020)\", \"Paper Title\": \"Oops!: Predicting Unintentional Action in Video (CVPR 2020)\", \"Paper URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"GitHub URL\": \"https://github.com/cvlab-columbia/oops\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/oops-predicting-unintentional-action-in-video\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf\", \"Semantic Scholar Corpus ID\": 208291335, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"CC BY-NC-SA 4.0\", \"License URL\": \"https://oops.cs.columbia.edu/data/\"}], \"Creators\": [\"Columbia University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 50.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"hd-vila-100m\", \"Collection\": \"hd-vila-100m\", \"Collection URL\": \"https://arxiv.org/abs/2111.10337\", \"Dataset Name\": \"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"Paper Title\": \"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\", \"Paper URL\": \"https://arxiv.org/abs/2111.10337\", \"GitHub URL\": \"https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/advancing-high-resolution-video-language/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2111.10337\", \"Semantic Scholar Corpus ID\": 244462849, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md\"}], \"Creators\": [\"Microsoft\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 371.5, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moments-in-time-dataset\", \"Collection\": \"moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Dataset Name\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\\n\", \"Paper Title\": \"Moments in Time Dataset: one million videos for event understanding (TPAMI 2019)\\n\", \"Paper URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1801.03150.pdf\", \"Semantic Scholar Corpus ID\": 11868155, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moments.csail.mit.edu/\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"ferv39k-dataset\", \"Collection\": \"ferv39k-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2203.09463\", \"Dataset Name\": \"FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos (CVPR 2022)\\n\", \"Paper Title\": \"FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos (CVPR 2022)\\n\", \"Paper URL\": \"https://arxiv.org/abs/2203.09463\", \"GitHub URL\": \"https://github.com/wangyanckxx/FERV39k\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ferv39k-a-large-scale-multi-scene-dataset-for\", \"ArXiv URL\": \"https://arxiv.org/abs/2203.09463\", \"Semantic Scholar Corpus ID\": 247518747, \"Year Released\": \"2022\", \"Text Sources\": [\"undisclosed web\"], \"Video Task\": \"Video Classification\", \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://wangyanckxx.github.io/Proj_CVPR2022_FERV39k.html\"}], \"Creators\": [\"Fudan University\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 16.47, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"hacs-dataset\", \"Collection\": \"hacs-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1712.09374\", \"Dataset Name\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\\n\", \"Paper Title\": \"HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization\\n\", \"Paper URL\": \"https://arxiv.org/abs/1712.09374\", \"GitHub URL\": \"https://github.com/hangzhaomit/HACS-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hacs\", \"ArXiv URL\": \"https://arxiv.org/abs/1712.09374\", \"Semantic Scholar Corpus ID\": 68049510, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/hangzhaomit/HACS-dataset?tab=readme-ov-file#request-testing-videos-and-missing-videos-new\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"Dartmouth University\", \"UIUC\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mmact\", \"Collection\": \"mmact\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"Dataset Name\": \"MMAct\", \"Paper Title\": \"MMAct\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mmact\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/9009579\", \"Semantic Scholar Corpus ID\": 207980205, \"Year Released\": \"2019\", \"Text Sources\": [\"human\"], \"Video Task\": \"Temporal Localization, Egocentric View, Action Recognition\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://mmact19.github.io/2019/\"}], \"Creators\": [\"The Hong Kong University of Science and Technology\", \"Alibaba Group\"], \"Countries\": [\"Hong Kong\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 100.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"titan\", \"Collection\": \"titan\", \"Collection URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"Dataset Name\": \"TITAN: Future Forecast using Action Priors (CVPR 2020)\\n\", \"Paper Title\": \"TITAN: Future Forecast using Action Priors (CVPR 2020)\\n\", \"Paper URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/titan-future-forecast-using-action-priors\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf\", \"Semantic Scholar Corpus ID\": 214727763, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Non Commercial\", \"License URL\": \"https://usa.honda-ri.com/titan\"}], \"Creators\": [\"Honda Research Institute\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2.91, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mad\", \"Collection\": \"mad\", \"Collection URL\": \"https://arxiv.org/abs/2112.00431\", \"Dataset Name\": \"MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions\\n\", \"Paper Title\": \"MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions\\n\", \"Paper URL\": \"https://arxiv.org/abs/2112.00431\", \"GitHub URL\": \"https://github.com/Soldelli/MAD\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mad\", \"ArXiv URL\": \"https://arxiv.org/abs/2112.00431\", \"Semantic Scholar Corpus ID\": 244773187, \"Year Released\": \"2022\", \"Text Sources\": [\"undisclosed web\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdtUV3uweS0u7AHAMIJAL_dRRdZ5MHpJS3fdZVbhnVt-Yb4NA/viewform\"}], \"Creators\": [\"King Abdullah University of Science and Technology (KAUST)\"], \"Countries\": [\"Saudi Arabia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1207.3, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"ntu-rgbd\", \"Collection\": \"ntu-rgbd\", \"Collection URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"Dataset Name\": \"NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis (CVPR 2016, TPAMI 2019)\\n\", \"Paper Title\": \"NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis (CVPR 2016, TPAMI 2019)\\n\", \"Paper URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"GitHub URL\": \"https://github.com/shahroudy/NTURGB-D\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ntu-rgbd-a-large-scale-dataset-for-3d-human\", \"ArXiv URL\": \"https://arxiv.org/pdf/1604.02808.pdf\", \"Semantic Scholar Corpus ID\": 15928602, \"Year Released\": \"2016\", \"Text Sources\": [\"crowdsourced\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://rose1.ntu.edu.sg/dataset/actionRecognition/\"}], \"Creators\": [\"Nanyang Technological University\"], \"Countries\": [\"Singapore\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 74.1, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"imagenet-vid\", \"Collection\": \"imagenet-vid\", \"Collection URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"Dataset Name\": \"ImageNet VID\", \"Paper Title\": \"ImageNet VID\", \"Paper URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/sota/video-object-detection-on-imagenet-vid\", \"ArXiv URL\": \"https://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#\", \"Semantic Scholar Corpus ID\": 2930547, \"Year Released\": \"2017\", \"Text Sources\": [\"undisclosed web\"], \"Video Task\": \"Video Classifcation\", \"Licenses\": [{\"License\": \"CC BY-NC 4.0\", \"License URL\": \"https://www.image-net.org/challenges/LSVRC/2017/index.php\"}], \"Creators\": [\"Stanford University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 9.26, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"License Use (DataProvenance)\": \"non-commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"non-commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"how2\\n\", \"Collection\": \"how2\\n\", \"Collection URL\": \"https://arxiv.org/abs/1811.00347\", \"Dataset Name\": \"How2: A Large-scale Dataset for Multimodal Language Understanding (NeurIPS 2018)\\n\", \"Paper Title\": \"How2: A Large-scale Dataset for Multimodal Language Understanding (NeurIPS 2018)\\n\", \"Paper URL\": \"https://arxiv.org/abs/1811.00347\", \"GitHub URL\": \"https://github.com/srvk/how2-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/abs/1811.00347\", \"Semantic Scholar Corpus ID\": 53186236, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Various\", \"License URL\": \"https://github.com/srvk/how2-dataset?tab=readme-ov-file#how2-license\"}], \"Creators\": [\"Carnegie Mellon University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 2300.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"WebVid\", \"Collection\": \"WebVid\", \"Collection URL\": \"https://arxiv.org/abs/2104.00650\", \"Dataset Name\": \"WebVid\", \"Paper Title\": \"WebVid\", \"Paper URL\": \"https://arxiv.org/abs/2104.00650\", \"GitHub URL\": \"https://github.com/m-bain/webvid\", \"Hugging Face URL\": \"https://huggingface.co/datasets/TempoFunk/webvid-10M\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/webvid\", \"ArXiv URL\": \"https://arxiv.org/abs/2104.00650\", \"Semantic Scholar Corpus ID\": 232478955, \"Year Released\": \"2021\", \"Text Sources\": [\"undisclosed web\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/m-bain/webvid/blob/main/TERMS.md\"}], \"Creators\": [\"University of Oxford\", \"CNRS\"], \"Countries\": [\"United Kingdom\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13000.0, \"Taken Down\": \"True\", \"Video Sources\": [\"undisclosed web\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"moviegraphs\", \"Collection\": \"moviegraphs\", \"Collection URL\": \"https://arxiv.org/pdf/1712.06761\", \"Dataset Name\": \"MovieGraphs\", \"Paper Title\": \"MovieGraphs\", \"Paper URL\": \"https://arxiv.org/pdf/1712.06761\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/moviegraphs\", \"ArXiv URL\": \"https://arxiv.org/pdf/1712.06761\", \"Semantic Scholar Corpus ID\": 4856028, \"Year Released\": \"2018\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"http://moviegraphs.cs.toronto.edu/download.html\"}], \"Creators\": [\"Vector Institute for Artificial Intelligence\", \"Montreal Institute of Learning Algorithms (Mila)\", \"University of Toronto\"], \"Countries\": [\"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 93.9, \"Taken Down\": \"True\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"charades-ego\", \"Collection\": \"charades-ego\", \"Collection URL\": \"https://arxiv.org/abs/1804.09627\", \"Dataset Name\": \"Charades-Ego: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018)\", \"Paper Title\": \"Charades-Ego: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018)\", \"Paper URL\": \"https://arxiv.org/abs/1804.09627\", \"GitHub URL\": \"https://github.com/gsig/actor-observer\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/search?q_meta=&q_type=&q=Actor+and+Observer%3A+Joint+Modeling+of+First+and+Third-Person+Videos\", \"ArXiv URL\": \"https://arxiv.org/abs/1804.09627\", \"Semantic Scholar Corpus ID\": 4562167, \"Year Released\": \"2018\", \"Text Sources\": [\"human\"], \"Video Task\": \"Egocentric View\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://prior.allenai.org/projects/data/charades-ego/license.txt\"}], \"Creators\": [\"Allen Institute for AI\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 69.33, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"uav-human\", \"Collection\": \"uav-human\", \"Collection URL\": \"https://arxiv.org/abs/2104.00946\", \"Dataset Name\": \"UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles\\n\", \"Paper Title\": \"UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles\\n\", \"Paper URL\": \"https://arxiv.org/abs/2104.00946\", \"GitHub URL\": \"https://github.com/sutdcv/UAV-Human\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/uav-human-a-large-benchmark-for-human\", \"ArXiv URL\": \"https://arxiv.org/abs/2104.00946\", \"Semantic Scholar Corpus ID\": 233004700, \"Year Released\": \"2021\", \"Text Sources\": [\"human\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://sutdcv.github.io/uav-human-web/\"}], \"Creators\": [\"Shandong University\", \"Singapore University of Technology and Design\"], \"Countries\": [\"Singapore\", \"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 18.34, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mpii-cooking2\", \"Collection\": \"mpii-cooking2\", \"Collection URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"Dataset Name\": \"MPII Cooking 2\", \"Paper Title\": \"MPII Cooking 2\", \"Paper URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mpii-cooking-2-dataset\", \"ArXiv URL\": \"https://arxiv.org/pdf/1502.06648.pdf\", \"Semantic Scholar Corpus ID\": 14036544, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Video Task\": \"Action Segmentation\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-2-dataset/\"}], \"Creators\": [\"Max Planck Institute for Informatics\", \"Saarland University\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 27.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"lsmdc\", \"Collection\": \"lsmdc\", \"Collection URL\": \"https://arxiv.org/pdf/1605.03705\", \"Dataset Name\": \"LSMDC\", \"Paper Title\": \"LSMDC\", \"Paper URL\": \"https://arxiv.org/pdf/1605.03705\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/movie-description\", \"ArXiv URL\": \"https://arxiv.org/pdf/1605.03705\", \"Semantic Scholar Corpus ID\": 18217052, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://datasets.d2.mpi-inf.mpg.de/movieDescription/protected/lsmdc2016/README.txt\"}], \"Creators\": [\"UC Berkeley\", \"Disney Research\", \"Universite de Montreal\", \"Polytechnique Montr\\u00e9al\", \"Universit\\u00e9 de Sherbrooke\", \"Twitter\"], \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"mpii-cooking\", \"Collection\": \"mpii-cooking\", \"Collection URL\": \"https://arxiv.org/abs/1502.06648\", \"Dataset Name\": \"MPII-Cooking: Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data (IJCV 2015)\\n\", \"Paper Title\": \"MPII-Cooking: Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data (IJCV 2015)\\n\", \"Paper URL\": \"https://arxiv.org/abs/1502.06648\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/recognizing-fine-grained-and-composite\", \"ArXiv URL\": \"https://arxiv.org/abs/1502.06648\", \"Semantic Scholar Corpus ID\": 14036544, \"Year Released\": \"2015\", \"Text Sources\": [\"human\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset\"}], \"Creators\": [\"Max Planck Institute for Informatics\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 27.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"academic-only\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"academic-only\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Non-Commercial/Academic\"}, {\"Unique Dataset Identifier\": \"violin\", \"Collection\": \"violin\", \"Collection URL\": \"https://arxiv.org/abs/2003.11618\", \"Dataset Name\": \"VIOLIN\", \"Paper Title\": \"VIOLIN\", \"Paper URL\": \"https://arxiv.org/abs/2003.11618\", \"GitHub URL\": \"https://github.com/jimmy646/violin\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/violin\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.11618\", \"Semantic Scholar Corpus ID\": 214668012, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 582.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"homage\", \"Collection\": \"homage\", \"Collection URL\": \"https://arxiv.org/abs/2105.05226\", \"Dataset Name\": \"HOMAGE: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021)\", \"Paper Title\": \"HOMAGE: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2105.05226\", \"GitHub URL\": \"https://github.com/nishantrai18/homage\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/home-action-genome-cooperative-compositional\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.05226\", \"Semantic Scholar Corpus ID\": 234357543, \"Year Released\": \"2021\", \"Text Sources\": [\"human\"], \"Video Task\": \"Egocentric View\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Stanford University\", \"Panasonic Corporation\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"mpii-md\", \"Collection\": \"mpii-md\", \"Collection URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"Dataset Name\": \"MPII-MD: A Dataset for Movie Description\\n\", \"Paper Title\": \"MPII-MD: A Dataset for Movie Description\\n\", \"Paper URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1501.02530.pdf\", \"Semantic Scholar Corpus ID\": 15184723, \"Year Released\": \"2017\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Max Planck Institute for Informatics\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 56.5, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"movie-net\", \"Collection\": \"movie-net\", \"Collection URL\": \"https://arxiv.org/abs/2007.10937\", \"Dataset Name\": \"MovieNet\", \"Paper Title\": \"MovieNet\", \"Paper URL\": \"https://arxiv.org/abs/2007.10937\", \"GitHub URL\": \"https://github.com/movienet/movienet-tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/movienet\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.10937\", \"Semantic Scholar Corpus ID\": 220665753, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"tiny-virat\", \"Collection\": \"tiny-virat\", \"Collection URL\": \"https://arxiv.org/abs/2007.07355\", \"Dataset Name\": \"TinyVIRAT: Low-resolution Video Action Recognition\", \"Paper Title\": \"TinyVIRAT: Low-resolution Video Action Recognition\", \"Paper URL\": \"https://arxiv.org/abs/2007.07355\", \"GitHub URL\": \"https://github.com/UgurDemir/Tiny-VIRAT\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tinyvirat\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.07355\", \"Semantic Scholar Corpus ID\": 220525685, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Central Florida\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.83, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"msa\", \"Collection\": \"msa\", \"Collection URL\": \"https://arxiv.org/abs/1910.11009\", \"Dataset Name\": \"MSA\", \"Paper Title\": \"MSA\", \"Paper URL\": \"https://arxiv.org/abs/1910.11009\", \"GitHub URL\": \"https://github.com/ycxioooong/MovieSynopsisAssociation\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-graph-based-framework-to-bridge-movies-and-1\", \"ArXiv URL\": \"https://arxiv.org/abs/1910.11009\", \"Semantic Scholar Corpus ID\": 204852218, \"Year Released\": \"2019\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Chinese University of Hong Kong\", \"UC Berkeley\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 516.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"lemma-dataset\", \"Collection\": \"lemma-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2007.15781\", \"Dataset Name\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\\n\", \"Paper Title\": \"LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020)\\n\", \"Paper URL\": \"https://arxiv.org/abs/2007.15781\", \"GitHub URL\": \"https://github.com/Buzz-Beater/LEMMA\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/lemma\", \"ArXiv URL\": \"https://arxiv.org/abs/2007.15781\", \"Semantic Scholar Corpus ID\": 220634784, \"Year Released\": \"2020\", \"Text Sources\": [\"crowdsourced\", \"human\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"UCLA\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 10.8, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\", \"human\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"crosstask\", \"Collection\": \"crosstask\", \"Collection URL\": \"https://arxiv.org/abs/1903.08225\", \"Dataset Name\": \"CrossTask\", \"Paper Title\": \"CrossTask\", \"Paper URL\": \"https://arxiv.org/abs/1903.08225\", \"GitHub URL\": \"https://github.com/DmZhukov/CrossTask\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/crosstask\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.08225\", \"Semantic Scholar Corpus ID\": 84187266, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Middle East Technical University\", \"Inria\", \"PSL University - Universit\\u00e9 PSL\", \"University of Michigan\", \"CIIRC\"], \"Countries\": [\"Czech Republic\", \"France\", \"Turkey\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 376.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"summe\", \"Collection\": \"summe\", \"Collection URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Dataset Name\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper Title\": \"SumMe: Creating Summaries from User Videos (ECCV 2014)\", \"Paper URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/summe\", \"ArXiv URL\": \"https://link.springer.com/chapter/10.1007/978-3-319-10584-0_33\", \"Semantic Scholar Corpus ID\": 2111093, \"Year Released\": \"2014\", \"Text Sources\": [\"crowdsourced\"], \"Video Task\": \"Video Summarization\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"ETH Z\\u00fcrich\", \"KU Leuven\", \"upicto GmbH\"], \"Countries\": [\"Belgium\", \"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1.11, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"multi-moments-in-time-dataset\", \"Collection\": \"multi-moments-in-time-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Dataset Name\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\\n\", \"Paper Title\": \"Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding\\n\", \"Paper URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/pdf/1911.00232.pdf\", \"Semantic Scholar Corpus ID\": 207780280, \"Year Released\": \"2021\", \"Text Sources\": [\"crowdsourced\", \"undisclosed web\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Massachusetts Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 833.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\", \"undisclosed web\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"howto100m\", \"Collection\": \"howto100m\", \"Collection URL\": \"https://arxiv.org/abs/1906.03327\", \"Dataset Name\": \"HowTo100M\", \"Paper Title\": \"HowTo100M\", \"Paper URL\": \"https://arxiv.org/abs/1906.03327\", \"GitHub URL\": \"https://github.com/antoine77340/howto100m\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/howto100m\", \"ArXiv URL\": \"https://arxiv.org/abs/1906.03327\", \"Semantic Scholar Corpus ID\": 182952863, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"CIIRC\", \"Czech Technical University\"], \"Countries\": [\"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 134472.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"youtube-8m\", \"Collection\": \"youtube-8m\", \"Collection URL\": \"https://arxiv.org/abs/1609.08675\", \"Dataset Name\": \"Youtube-8M: A Large-Scale Video Classification Benchmark\\n\", \"Paper Title\": \"Youtube-8M: A Large-Scale Video Classification Benchmark\\n\", \"Paper URL\": \"https://arxiv.org/abs/1609.08675\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-8m\", \"ArXiv URL\": \"https://arxiv.org/abs/1609.08675\", \"Semantic Scholar Corpus ID\": 11241677, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Classification\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Google Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 350000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"youcook\", \"Collection\": \"youcook\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"Dataset Name\": \"YouCook: A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching (CVPR 2013)\\n\", \"Paper Title\": \"YouCook: A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching (CVPR 2013)\\n\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youcook\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 12284555, \"Year Released\": \"2013\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University at Buffalo\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"rare-act-dataset\", \"Collection\": \"rare-act-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Dataset Name\": \"RareAct: A video dataset of unusual interactions\\n\", \"Paper Title\": \"RareAct: A video dataset of unusual interactions\\n\", \"Paper URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"GitHub URL\": \"https://github.com/antoine77340/RareAct\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/rareact\", \"ArXiv URL\": \"https://arxiv.org/pdf/2008.01018.pdf\", \"Semantic Scholar Corpus ID\": 220936243, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Oxford\", \"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"CIIRC\", \"Czech Technical University\"], \"Countries\": [\"United Kingdom\", \"France\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 21.13, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"hollywood2-dataset\", \"Collection\": \"hollywood2-dataset\", \"Collection URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"Dataset Name\": \"HOLLYWOOD2: Actions in Context (CVPR 2009)\", \"Paper Title\": \"HOLLYWOOD2: Actions in Context (CVPR 2009)\", \"Paper URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf\", \"Semantic Scholar Corpus ID\": 3155054, \"Year Released\": \"<2013\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Inria\"], \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 20.1, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"qfvs\", \"Collection\": \"qfvs\", \"Collection URL\": \"https://arxiv.org/abs/1707.04960\", \"Dataset Name\": \"QFVS: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach (CVPR 2017)\", \"Paper Title\": \"QFVS: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach (CVPR 2017)\", \"Paper URL\": \"https://arxiv.org/abs/1707.04960\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/query-focused-video-summarization-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/1707.04960\", \"Semantic Scholar Corpus ID\": 2774608, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Video Task\": \"Video Summarization\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Central Florida\", \"University of Alabama\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 20.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"stroygraphs\", \"Collection\": \"stroygraphs\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"Dataset Name\": \"StoryGraphs\", \"Paper Title\": \"StoryGraphs\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"GitHub URL\": \"https://github.com/makarandtapaswi/StoryGraphs_CVPR2014\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/storygraphs-visualizing-character\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/papers/Tapaswi_StoryGraphs_Visualizing_Character_2014_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 1055956, \"Year Released\": \"2014\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Karlsruhe Institute of Technology\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.3, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"videostory\", \"Collection\": \"videostory\", \"Collection URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"Dataset Name\": \"VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events\", \"Paper Title\": \"VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events\", \"Paper URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf\", \"Semantic Scholar Corpus ID\": 28203, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Amsterdam\"], \"Countries\": [\"Netherlands\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 743.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"collective\", \"Collection\": \"collective\", \"Collection URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"Dataset Name\": \"Collective: What are they doing? : Collective activity classification using spatio-temporal relationship among people\", \"Paper Title\": \"Collective: What are they doing? : Collective activity classification using spatio-temporal relationship among people\", \"Paper URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://cvgl.stanford.edu/papers/Wongun_CollectiveActivityRecognition09.pdf\", \"Semantic Scholar Corpus ID\": 5925915, \"Year Released\": \"<2013\", \"Text Sources\": [\"human\"], \"Video Task\": \"Group Activity Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Michigan\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.1, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"msr-vtt\", \"Collection\": \"msr-vtt\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"Dataset Name\": \"MSR-VTT\", \"Paper Title\": \"MSR-VTT\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/msr-vtt\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7780940\", \"Semantic Scholar Corpus ID\": 206594535, \"Year Released\": \"2016\", \"Text Sources\": [\"undisclosed web\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 41.2, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"movieqa\", \"Collection\": \"movieqa\", \"Collection URL\": \"https://arxiv.org/abs/1512.02902\", \"Dataset Name\": \"MovieQA\", \"Paper Title\": \"MovieQA\", \"Paper URL\": \"https://arxiv.org/abs/1512.02902\", \"GitHub URL\": \"https://github.com/makarandtapaswi/MovieQA_CVPR2016/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/movieqa\", \"ArXiv URL\": \"https://arxiv.org/abs/1512.02902\", \"Semantic Scholar Corpus ID\": 1017389, \"Year Released\": \"2015\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"University of Toronto\"], \"Countries\": [\"Germany\", \"Canada\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 381.0, \"Taken Down\": \"True\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"ucf101-dataset\", \"Collection\": \"ucf101-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1212.0402\", \"Dataset Name\": \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\\n\", \"Paper Title\": \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\\n\", \"Paper URL\": \"https://arxiv.org/abs/1212.0402\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/ucf101\", \"ArXiv URL\": \"https://arxiv.org/abs/1212.0402\", \"Semantic Scholar Corpus ID\": 7197134, \"Year Released\": \"<2013\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Central Florida\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 26.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"haa500-dataset\", \"Collection\": \"haa500-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2009.05224\", \"Dataset Name\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper Title\": \"HAA500: Human-Centric Atomic Action Dataset with Curated Videos (ICCV 2021)\", \"Paper URL\": \"https://arxiv.org/abs/2009.05224\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/haa500-human-centric-atomic-action-dataset\", \"ArXiv URL\": \"https://arxiv.org/abs/2009.05224\", \"Semantic Scholar Corpus ID\": 221640805, \"Year Released\": \"2020\", \"Text Sources\": [\"human\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Carnegie Mellon University\", \"The Hong Kong University of Science and Technology\", \"Princeton University\", \"Kuaishou Technology\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 5.48, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"vtw\", \"Collection\": \"vtw\", \"Collection URL\": \"https://arxiv.org/abs/1608.07068\", \"Dataset Name\": \"VTW\", \"Paper Title\": \"VTW\", \"Paper URL\": \"https://arxiv.org/abs/1608.07068\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/title-generation-for-user-generated-videos\", \"ArXiv URL\": \"https://arxiv.org/abs/1608.07068\", \"Semantic Scholar Corpus ID\": 6155397, \"Year Released\": \"2016\", \"Text Sources\": [\"undisclosed web\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Tsinghua University\", \"Stanford University\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 213.2, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"moviescenes\", \"Collection\": \"moviescenes\", \"Collection URL\": \"https://arxiv.org/abs/2004.02678\", \"Dataset Name\": \"MovieScenes\", \"Paper Title\": \"MovieScenes\", \"Paper URL\": \"https://arxiv.org/abs/2004.02678\", \"GitHub URL\": \"https://github.com/AnyiRao/SceneSeg\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-local-to-global-approach-to-multi-modal\", \"ArXiv URL\": \"https://arxiv.org/abs/2004.02678\", \"Semantic Scholar Corpus ID\": 214802984, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Chinese University of Hong Kong\", \"UC Berkeley\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 250.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"soa-dataset\", \"Collection\": \"soa-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/1904.11451\", \"Dataset Name\": \"Scenes-objects-actions: A multi-task, multi-label video dataset\", \"Paper Title\": \"Scenes-objects-actions: A multi-task, multi-label video dataset\", \"Paper URL\": \"https://arxiv.org/pdf/1904.11451\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/scenes-objects-actions-a-multi-task-multi\", \"ArXiv URL\": \"https://arxiv.org/pdf/1904.11451\", \"Semantic Scholar Corpus ID\": 52968009, \"Year Released\": \"2018\", \"Text Sources\": [\"facebook\"], \"Video Task\": \"Video Classification\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1561.1, \"Taken Down\": \"False\", \"Video Sources\": [\"facebook\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"mars\", \"Collection\": \"mars\", \"Collection URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Dataset Name\": \"Mars\", \"Paper Title\": \"Mars\", \"Paper URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/mars\", \"ArXiv URL\": \"https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_52.pdf\", \"Semantic Scholar Corpus ID\": 2214158, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Video Task\": \"Video Segmentation, Video Classification\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft\", \"Tsinghua University\", \"The University of Texas at San Antonio\", \"Peking University\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.24, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"pku-mmd-dataset\", \"Collection\": \"pku-mmd-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1703.07475\", \"Dataset Name\": \"PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop)\", \"Paper Title\": \"PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop)\", \"Paper URL\": \"https://arxiv.org/abs/1703.07475\", \"GitHub URL\": \"https://struct002.github.io/PKUMMD/\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/pku-mmd-a-large-scale-benchmark-for\", \"ArXiv URL\": \"https://arxiv.org/abs/1703.07475\", \"Semantic Scholar Corpus ID\": 1904265, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Microsoft Research\", \"Peking University\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 50.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"kinetics-400\", \"Collection\": \"kinetics-400\", \"Collection URL\": \"https://arxiv.org/abs/1705.06950\", \"Dataset Name\": \"Kinetics 400\", \"Paper Title\": \"Kinetics 400\", \"Paper URL\": \"https://arxiv.org/abs/1705.06950\", \"GitHub URL\": \"https://github.com/cvdfoundation/kinetics-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics\", \"ArXiv URL\": \"https://arxiv.org/abs/1705.06950\", \"Semantic Scholar Corpus ID\": 27300853, \"Year Released\": \"2017\", \"Text Sources\": [\"human\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"DeepMind\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 850.68, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"crosstask\\n\", \"Collection\": \"crosstask\\n\", \"Collection URL\": \"https://arxiv.org/abs/1903.08225\", \"Dataset Name\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\\n\", \"Paper Title\": \"CrossTask: weakly supervised learning from instructional videos (CVPR 2019)\\n\", \"Paper URL\": \"https://arxiv.org/abs/1903.08225\", \"GitHub URL\": \"https://github.com/DmZhukov/CrossTask\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/crosstask\", \"ArXiv URL\": \"https://arxiv.org/abs/1903.08225\", \"Semantic Scholar Corpus ID\": 84187266, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"CIIRC\", \"Czech Technical University\", \"University of Michigan\"], \"Countries\": [\"France\", \"Turkey\", \"United States of America\", \"Czech Republic\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 376.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"apes\", \"Collection\": \"apes\", \"Collection URL\": \"https://arxiv.org/pdf/2106.01667\", \"Dataset Name\": \"Apes\", \"Paper Title\": \"Apes\", \"Paper URL\": \"https://arxiv.org/pdf/2106.01667\", \"GitHub URL\": \"https://github.com/fuankarion/audiovisual-person-search\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/apes-audiovisual-person-search-in-untrimmed/review/\", \"ArXiv URL\": \"https://arxiv.org/pdf/2106.01667\", \"Semantic Scholar Corpus ID\": 235313698, \"Year Released\": \"2021\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Video Classification\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Universidad de los Andes\", \"Adobe Research\", \"King Abdullah University of Science and Technology (KAUST)\"], \"Countries\": [\"Chile\", \"United States of America\", \"Saudi Arabia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 36.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"volleyball-vids\", \"Collection\": \"volleyball-vids\", \"Collection URL\": \"https://arxiv.org/abs/1511.06040\", \"Dataset Name\": \"Volleyball: A Hierarchical Deep Temporal Model for Group Activity Recognition\", \"Paper Title\": \"Volleyball: A Hierarchical Deep Temporal Model for Group Activity Recognition\", \"Paper URL\": \"https://arxiv.org/abs/1511.06040\", \"GitHub URL\": \"https://github.com/mostafa-saad/deep-activity-rec#dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/volleyball\", \"ArXiv URL\": \"https://arxiv.org/abs/1511.06040\", \"Semantic Scholar Corpus ID\": 8483403, \"Year Released\": \"2015\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Group Activity Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Simon Fraser University\"], \"Countries\": [\"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.1, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"mimetics-dataset\", \"Collection\": \"mimetics-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1912.07249\", \"Dataset Name\": \"Mimetics: Towards Understanding Human Actions Out of Context\\n\", \"Paper Title\": \"Mimetics: Towards Understanding Human Actions Out of Context\\n\", \"Paper URL\": \"https://arxiv.org/abs/1912.07249\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/mimetics-towards-understanding-human-actions\", \"ArXiv URL\": \"https://arxiv.org/abs/1912.07249\", \"Semantic Scholar Corpus ID\": 209376248, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"Naver\"], \"Countries\": [\"South Korea\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.99, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"cacd\", \"Collection\": \"cacd\", \"Collection URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"Dataset Name\": \"CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples (CVPR 2022)\\n\", \"Paper Title\": \"CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples (CVPR 2022)\\n\", \"Paper URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"GitHub URL\": \"https://github.com/MartinXM/CDAD\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf\", \"Semantic Scholar Corpus ID\": 251035434, \"Year Released\": \"2022\", \"Text Sources\": [\"crowdsourced\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"The Hong Kong Polytechnic University\", \"Alibaba Group\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 215.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"kinetics-600\", \"Collection\": \"kinetics-600\", \"Collection URL\": \"https://arxiv.org/abs/1808.01340\", \"Dataset Name\": \"Kinetics 600\", \"Paper Title\": \"Kinetics 600\", \"Paper URL\": \"https://arxiv.org/abs/1808.01340\", \"GitHub URL\": \"https://github.com/cvdfoundation/kinetics-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics-600\", \"ArXiv URL\": \"https://arxiv.org/abs/1808.01340\", \"Semantic Scholar Corpus ID\": 51927456, \"Year Released\": \"2018\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"DeepMind\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1376.52, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"queryd\", \"Collection\": \"queryd\", \"Collection URL\": \"https://arxiv.org/abs/2011.11071\", \"Dataset Name\": \"QuerYD\", \"Paper Title\": \"QuerYD\", \"Paper URL\": \"https://arxiv.org/abs/2011.11071\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/queryd\", \"ArXiv URL\": \"https://arxiv.org/abs/2011.11071\", \"Semantic Scholar Corpus ID\": 261006321, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"youdescribe\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 207.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\", \"youdescribe\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"kinetics-700\", \"Collection\": \"kinetics-700\", \"Collection URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"Dataset Name\": \"Kinetics-700\", \"Paper Title\": \"Kinetics-700\", \"Paper URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/kinetics-700\", \"ArXiv URL\": \"https://arxiv.org/pdf/2010.10864.pdf\", \"Semantic Scholar Corpus ID\": 196831809, \"Year Released\": \"2020\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Unspecified\", \"License URL\": \"\"}], \"Creators\": [\"DeepMind\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1805.56, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"unspecified\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"unspecified\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Unspecified\"}, {\"Unique Dataset Identifier\": \"20BN-jester\\n\", \"Collection\": \"20BN-jester\\n\", \"Collection URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"Dataset Name\": \"20BN-jester: The Jester Dataset: A Large-Scale Video Dataset of Human Gestures (ICCVW 2019)\\n\", \"Paper Title\": \"20BN-jester: The Jester Dataset: A Large-Scale Video Dataset of Human Gestures (ICCVW 2019)\\n\", \"Paper URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf\", \"Semantic Scholar Corpus ID\": 208010438, \"Year Released\": \"2019\", \"Text Sources\": [\"crowdsourced\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://developer.qualcomm.com/software/ai-datasets/jester\"}], \"Creators\": [\"Twenty Billion Neurons GmbH\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13.0, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"charades\", \"Collection\": \"charades\", \"Collection URL\": \"https://arxiv.org/abs/1604.01753\", \"Dataset Name\": \"Charades\", \"Paper Title\": \"Charades\", \"Paper URL\": \"https://arxiv.org/abs/1604.01753\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/charades\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.01753\", \"Semantic Scholar Corpus ID\": 18061547, \"Year Released\": \"2016\", \"Text Sources\": [\"human\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://huggingface.co/datasets/HuggingFaceM4/charades#licensing-information\"}], \"Creators\": [\"Carnegie Mellon University\", \"Inria\", \"University of Washington\", \"Allen Institute for AI\"], \"Countries\": [\"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 82.3, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"sports1M-dataset\\n\", \"Collection\": \"sports1M-dataset\\n\", \"Collection URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"Dataset Name\": \"Sports-1M: Large-scale Video Classification with Convolutional Neural Networks\\n\", \"Paper Title\": \"Sports-1M: Large-scale Video Classification with Convolutional Neural Networks\\n\", \"Paper URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"GitHub URL\": \"https://github.com/gtoderici/sports-1m-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/large-scale-video-classification-with-1\", \"ArXiv URL\": \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf\", \"Semantic Scholar Corpus ID\": 206592218, \"Year Released\": \"2014\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"CC BY 3.0\", \"License URL\": \"https://github.com/gtoderici/sports-1m-dataset\"}], \"Creators\": [\"Stanford University\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 105761.41, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"youcook-2\", \"Collection\": \"youcook-2\", \"Collection URL\": \"https://arxiv.org/abs/1703.09788\", \"Dataset Name\": \"YouCook2\", \"Paper Title\": \"YouCook2\", \"Paper URL\": \"https://arxiv.org/abs/1703.09788\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youcook2\", \"ArXiv URL\": \"https://arxiv.org/abs/1703.09788\", \"Semantic Scholar Corpus ID\": 19713015, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"http://youcook2.eecs.umich.edu/static/YouCookII/LICENSE_YOUCOOK2.txt\"}], \"Creators\": [\"University of Rochester\", \"University of Michigan\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 175.6, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ego-4d\", \"Collection\": \"ego-4d\", \"Collection URL\": \"https://arxiv.org/abs/2110.07058\", \"Dataset Name\": \"Ego4D: Around the World in 3,000 Hours of Egocentric Video\\n\", \"Paper Title\": \"Ego4D: Around the World in 3,000 Hours of Egocentric Video\\n\", \"Paper URL\": \"https://arxiv.org/abs/2110.07058\", \"GitHub URL\": \"https://github.com/EGO4D/forecasting\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ego4d-around-the-world-in-3000-hours-of\", \"ArXiv URL\": \"https://arxiv.org/abs/2110.07058\", \"Semantic Scholar Corpus ID\": 238856888, \"Year Released\": \"2022\", \"Text Sources\": [\"human\"], \"Video Task\": \"Egocentric View\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://ego4d-data.org/pdfs/Ego4D-Licenses-Draft.pdf\"}], \"Creators\": [\"Facebook AI Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3670.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"omnisource-web-dataset\", \"Collection\": \"omnisource-web-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2003.13042\", \"Dataset Name\": \"OmniSource Web Dataset\", \"Paper Title\": \"OmniSource Web Dataset\", \"Paper URL\": \"https://arxiv.org/abs/2003.13042\", \"GitHub URL\": \"https://github.com/open-mmlab/mmaction\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/omni-sourced-webly-supervised-learning-for#code\", \"ArXiv URL\": \"https://arxiv.org/abs/2003.13042\", \"Semantic Scholar Corpus ID\": 214714240, \"Year Released\": \"2020\", \"Text Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"Video Task\": \"Video Classification\", \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/open-mmlab/mmaction?tab=readme-ov-file#license\"}], \"Creators\": [\"The Chinese University of Hong Kong\"], \"Countries\": [\"China\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 13333.0, \"Taken Down\": \"False\", \"Video Sources\": [\"google videos\", \"instagram\", \"youtube\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hollywood-extended\", \"Collection\": \"hollywood-extended\", \"Collection URL\": \"https://arxiv.org/pdf/1407.1208\", \"Dataset Name\": \"Hollywood Extended\", \"Paper Title\": \"Hollywood Extended\", \"Paper URL\": \"https://arxiv.org/pdf/1407.1208\", \"GitHub URL\": \"https://github.com/piotr-bojanowski/action-ordering\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/weakly-supervised-action-labeling-in-videos\", \"ArXiv URL\": \"https://arxiv.org/pdf/1407.1208\", \"Semantic Scholar Corpus ID\": 9342651, \"Year Released\": \"2014\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Action Segmentation, Action Recognition\", \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/piotr-bojanowski/action-ordering/blob/master/LICENSE\"}], \"Creators\": [\"Inria\"], \"Countries\": [\"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 8.75, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"eev-dataset\", \"Collection\": \"eev-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2001.05488\", \"Dataset Name\": \"EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video\", \"Paper Title\": \"EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video\", \"Paper URL\": \"https://arxiv.org/abs/2001.05488\", \"GitHub URL\": \"https://github.com/google-research-datasets/eev\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://arxiv.org/abs/2001.05488\", \"Semantic Scholar Corpus ID\": 210701992, \"Year Released\": \"2020\", \"Text Sources\": [\"undisclosed web\"], \"Video Task\": \"Video Classification\", \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://github.com/google-research-datasets/eev?tab=readme-ov-file#license\"}], \"Creators\": [\"Google Research\", \"California Institute of Technology\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 370.0, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"narrated-instruction-vids\\n\", \"Collection\": \"narrated-instruction-vids\\n\", \"Collection URL\": \"https://arxiv.org/abs/1506.09215\", \"Dataset Name\": \"Narrated Instruction Videos\", \"Paper Title\": \"Narrated Instruction Videos\", \"Paper URL\": \"https://arxiv.org/abs/1506.09215\", \"GitHub URL\": \"https://github.com/jalayrac/instructionVideos\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/youtube-inria-instructional\", \"ArXiv URL\": \"https://arxiv.org/abs/1506.09215\", \"Semantic Scholar Corpus ID\": 2617244, \"Year Released\": \"2016\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/jalayrac/instructionVideos?tab=readme-ov-file#license\"}], \"Creators\": [\"CNRS\", \"Ecole Normale Sup\\u00e9rieure\", \"Inria\", \"International Institute of Information Technology - Hyderabad\"], \"Countries\": [\"United States of America\", \"India\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"project-aria-digital-twin-dataset\", \"Collection\": \"project-aria-digital-twin-dataset\", \"Collection URL\": \"https://arxiv.org/abs/2306.06362\", \"Dataset Name\": \"Aria Digital Twin\", \"Paper Title\": \"Aria Digital Twin\", \"Paper URL\": \"https://arxiv.org/abs/2306.06362\", \"GitHub URL\": \"https://github.com/facebookresearch/projectaria_tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/aria-digital-twin-a-new-benchmark-dataset-for\", \"ArXiv URL\": \"https://arxiv.org/abs/2306.06362\", \"Semantic Scholar Corpus ID\": 261243365, \"Year Released\": \"2023\", \"Text Sources\": [\"human\"], \"Video Task\": \"Egocentric View\", \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/facebookresearch/projectaria_tools/blob/main/LICENSE\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 6.6, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"tvsum\", \"Collection\": \"tvsum\", \"Collection URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"Dataset Name\": \"TVSum: Summarizing web videos using titles (CVPR 2015)\", \"Paper Title\": \"TVSum: Summarizing web videos using titles (CVPR 2015)\", \"Paper URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"GitHub URL\": \"https://github.com/yalesong/tvsum\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tvsum-1\", \"ArXiv URL\": \"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf\", \"Semantic Scholar Corpus ID\": 7675635, \"Year Released\": \"2015\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Summarization\", \"Licenses\": [{\"License\": \"CC BY 3.0\", \"License URL\": \"https://github.com/yalesong/tvsum\"}], \"Creators\": [\"Yahoo Labs\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 3.5, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"project-aria-dataset\", \"Collection\": \"project-aria-dataset\", \"Collection URL\": \"https://arxiv.org/pdf/2402.13349\", \"Dataset Name\": \"Aria Everyday Activities Dataset\", \"Paper Title\": \"Aria Everyday Activities Dataset\", \"Paper URL\": \"https://arxiv.org/pdf/2402.13349\", \"GitHub URL\": \"https://github.com/facebookresearch/projectaria_tools\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/aria-everyday-activities-dataset\", \"ArXiv URL\": \"https://arxiv.org/pdf/2402.13349\", \"Semantic Scholar Corpus ID\": 267770215, \"Year Released\": \"2024\", \"Text Sources\": [\"human\"], \"Video Task\": \"Egocentric View\", \"Licenses\": [{\"License\": \"Apache License 2.0\", \"License URL\": \"https://github.com/facebookresearch/Aria_data_tools/blob/main/LICENSE\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1400.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"multi-thumos-challenge\", \"Collection\": \"multi-thumos-challenge\", \"Collection URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Dataset Name\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\\n\", \"Paper Title\": \"MultiTHUMOS: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017)\\n\", \"Paper URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/multithumos\", \"ArXiv URL\": \"https://arxiv.org/pdf/1507.05738v3.pdf\", \"Semantic Scholar Corpus ID\": 3337929, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://ai.stanford.edu/~syyeung/resources/multithumos.zip\"}], \"Creators\": [\"Stanford University\", \"Carnegie Mellon University\", \"Simon Fraser University\"], \"Countries\": [\"United States of America\", \"Canada\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 30.0, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ava\", \"Collection\": \"ava\", \"Collection URL\": \"https://arxiv.org/pdf/1705.08421\", \"Dataset Name\": \"AVA\", \"Paper Title\": \"AVA\", \"Paper URL\": \"https://arxiv.org/pdf/1705.08421\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ava-a-video-dataset-of-spatio-temporally\", \"ArXiv URL\": \"https://arxiv.org/pdf/1705.08421\", \"Semantic Scholar Corpus ID\": 688013, \"Year Released\": \"2017\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Temporal Localization, Action Recognition\", \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://research.google.com/ava/\"}], \"Creators\": [\"Google Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 107.5, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"vatex\", \"Collection\": \"vatex\", \"Collection URL\": \"https://arxiv.org/abs/1904.03493\", \"Dataset Name\": \"VaTeX\", \"Paper Title\": \"VaTeX\", \"Paper URL\": \"https://arxiv.org/abs/1904.03493\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/vatex\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/vatex\", \"ArXiv URL\": \"https://arxiv.org/abs/1904.03493\", \"Semantic Scholar Corpus ID\": 102352148, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://eric-xw.github.io/vatex-website/index.html\"}], \"Creators\": [\"ByteDance AI Lab\", \"UC Santa Barbara\"], \"Countries\": [\"China\", \"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 114.58, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ava-dataset\", \"Collection\": \"ava-dataset\", \"Collection URL\": \"https://arxiv.org/abs/1901.01342\", \"Dataset Name\": \"AVA Active Speaker\", \"Paper Title\": \"AVA Active Speaker\", \"Paper URL\": \"https://arxiv.org/abs/1901.01342\", \"GitHub URL\": \"https://github.com/cvdfoundation/ava-dataset\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/ava-activespeaker\", \"ArXiv URL\": \"https://arxiv.org/abs/1901.01342\", \"Semantic Scholar Corpus ID\": 216211909, \"Year Released\": \"2019\", \"Text Sources\": [\"youtube\", \"movies\"], \"Video Task\": \"Video Classification\", \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://research.google.com/ava/download.html#ava_active_speaker_download\"}], \"Creators\": [\"Google Research\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 38.5, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\", \"movies\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"condensed-movies\", \"Collection\": \"condensed-movies\", \"Collection URL\": \"https://arxiv.org/pdf/2005.04208\", \"Dataset Name\": \"Condensed Movies\", \"Paper Title\": \"Condensed Movies\", \"Paper URL\": \"https://arxiv.org/pdf/2005.04208\", \"GitHub URL\": \"https://github.com/m-bain/CondensedMovies\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/condensed-movies\", \"ArXiv URL\": \"https://arxiv.org/pdf/2005.04208\", \"Semantic Scholar Corpus ID\": 218571391, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://www.robots.ox.ac.uk/~vgg/data/condensed-movies/#download\"}], \"Creators\": [\"University of Oxford\"], \"Countries\": [\"United Kingdom\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1270.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"hmdb-dataset\\n\", \"Collection\": \"hmdb-dataset\\n\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Dataset Name\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\\n\", \"Paper Title\": \"HMDB: A Large Video Database for Human Motion Recognition (ICCV 2011)\\n\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/hmdb51\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/6126543\", \"Semantic Scholar Corpus ID\": 206769852, \"Year Released\": \"<2013\", \"Text Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"}], \"Creators\": [\"Karlsruhe Institute of Technology\", \"Massachusetts Institute of Technology\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 7000.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\", \"prelinger archive\", \"undisclosed web\", \"youtube\", \"google videos\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"didemo\", \"Collection\": \"didemo\", \"Collection URL\": \"https://paperswithcode.com/dataset/didemo\", \"Dataset Name\": \"DiDeMo: Localizing Moments in Video with Temporal Language (EMNLP 2018)\\n\", \"Paper Title\": \"DiDeMo: Localizing Moments in Video with Temporal Language (EMNLP 2018)\\n\", \"Paper URL\": \"https://paperswithcode.com/dataset/didemo\", \"GitHub URL\": \"https://github.com/LisaAnne/TemporalLanguageRelease\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"\", \"ArXiv URL\": \"https://paperswithcode.com/dataset/didemo\", \"Semantic Scholar Corpus ID\": 52164739, \"Year Released\": \"2018\", \"Text Sources\": [\"flickr\"], \"Video Task\": \"Video and Language\", \"Licenses\": [{\"License\": \"BSD 2-Clause License\", \"License URL\": \"https://github.com/LisaAnne/TemporalLanguageRelease\"}], \"Creators\": [\"UC Berkeley\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 275.0, \"Taken Down\": \"False\", \"Video Sources\": [\"flickr\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"davis\", \"Collection\": \"davis\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"Dataset Name\": \"Davis\", \"Paper Title\": \"Davis\", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"GitHub URL\": \"https://github.com/fperazzi/davis\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/a-benchmark-dataset-and-evaluation\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7780454\", \"Semantic Scholar Corpus ID\": 3619941, \"Year Released\": \"2017\", \"Text Sources\": [\"undisclosed web\"], \"Video Task\": \"Video Segmentation, Video Classification\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/fperazzi/davis/blob/main/LICENSE\"}], \"Creators\": [\"ETH Z\\u00fcrich\", \"Disney Research\"], \"Countries\": [\"Switzerland\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 0.04, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"tgif\", \"Collection\": \"tgif\", \"Collection URL\": \"https://arxiv.org/abs/1604.02748\", \"Dataset Name\": \"TGIF\", \"Paper Title\": \"TGIF\", \"Paper URL\": \"https://arxiv.org/abs/1604.02748\", \"GitHub URL\": \"https://github.com/raingo/TGIF-Release\", \"Hugging Face URL\": \"https://huggingface.co/datasets/HuggingFaceM4/TGIF\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/tgif\", \"ArXiv URL\": \"https://arxiv.org/abs/1604.02748\", \"Semantic Scholar Corpus ID\": 6262415, \"Year Released\": \"2016\", \"Text Sources\": [\"tumblr\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://github.com/raingo/TGIF-Release\"}], \"Creators\": [\"University of Rochester\", \"Yahoo! Inc.\", \"AiCure\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 86.1, \"Taken Down\": \"False\", \"Video Sources\": [\"tumblr\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"20bn-something\", \"Collection\": \"20bn-something\", \"Collection URL\": \"https://arxiv.org/abs/1706.04261\", \"Dataset Name\": \"20BN-SOMETHING-SOMETHING: The \\\"something something\\\" video database for learning and evaluating visual common sense\\n\", \"Paper Title\": \"20BN-SOMETHING-SOMETHING: The \\\"something something\\\" video database for learning and evaluating visual common sense\\n\", \"Paper URL\": \"https://arxiv.org/abs/1706.04261\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/something-something-v2\", \"ArXiv URL\": \"https://arxiv.org/abs/1706.04261\", \"Semantic Scholar Corpus ID\": 834612, \"Year Released\": \"2017\", \"Text Sources\": [\"crowdsourced\"], \"Video Task\": \"Action Recognition\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://developer.qualcomm.com/software/ai-datasets/something-something\"}], \"Creators\": [\"Twenty Billion Neurons GmbH\"], \"Countries\": [\"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 121.46, \"Taken Down\": \"False\", \"Video Sources\": [\"crowdsourced\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 1, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 1, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"lsmdc-ordering\", \"Collection\": \"lsmdc-ordering\", \"Collection URL\": \"https://arxiv.org/pdf/2004.02205\", \"Dataset Name\": \"LSMDC Ordering\", \"Paper Title\": \"LSMDC Ordering\", \"Paper URL\": \"https://arxiv.org/pdf/2004.02205\", \"GitHub URL\": \"https://github.com/vivoutlaw/TCBP\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/deep-multimodal-feature-encoding-for-video\", \"ArXiv URL\": \"https://arxiv.org/pdf/2004.02205\", \"Semantic Scholar Corpus ID\": 214802821, \"Year Released\": \"2020\", \"Text Sources\": [\"movies\"], \"Video Task\": \"Video Classification, Action Recognition\", \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/vivoutlaw/tcbp/blob/master/LICENSE\"}], \"Creators\": [\"University of Toronto\", \"Karlsruhe Institute of Technology\", \"Inria\", \"Massachusetts Institute of Technology\"], \"Countries\": [\"Germany\", \"Canada\", \"United States of America\", \"France\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 158.0, \"Taken Down\": \"False\", \"Video Sources\": [\"movies\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"activitynet\", \"Collection\": \"activitynet\", \"Collection URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"Dataset Name\": \"ActivityNet \", \"Paper Title\": \"ActivityNet \", \"Paper URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"https://huggingface.co/datasets/Leyo/ActivityNet_Captions\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/activitynet\", \"ArXiv URL\": \"https://ieeexplore.ieee.org/document/7298698\", \"Semantic Scholar Corpus ID\": 1710722, \"Year Released\": \"2015\", \"Text Sources\": [\"undisclosed web\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/facebookresearch/ActivityNet-Entities/blob/main/LICENSE\"}], \"Creators\": [\"Universidad del Norte\", \"King Abdullah University of Science and Technology (KAUST)\"], \"Countries\": [\"Saudi Arabia\", \"Colombia\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 849.0, \"Taken Down\": \"False\", \"Video Sources\": [\"undisclosed web\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"breakfast\\n\", \"Collection\": \"breakfast\\n\", \"Collection URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"Dataset Name\": \"Breakfast\", \"Paper Title\": \"Breakfast\", \"Paper URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/dataset/breakfast\", \"ArXiv URL\": \"https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html\", \"Semantic Scholar Corpus ID\": 9621856, \"Year Released\": \"2014\", \"Text Sources\": [\"human\"], \"Video Task\": \"Video Captioning, Action Segmentation\", \"Licenses\": [{\"License\": \"CC BY 4.0\", \"License URL\": \"https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/\"}], \"Creators\": [\"Fraunhofer FKIE\", \"Brown University\"], \"Countries\": [\"United States of America\", \"Germany\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 77.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"spoken-moments\", \"Collection\": \"spoken-moments\", \"Collection URL\": \"https://arxiv.org/abs/2105.04489\", \"Dataset Name\": \"Spoken Moments\", \"Paper Title\": \"Spoken Moments\", \"Paper URL\": \"https://arxiv.org/abs/2105.04489\", \"GitHub URL\": \"\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual\", \"ArXiv URL\": \"https://arxiv.org/abs/2105.04489\", \"Semantic Scholar Corpus ID\": 233992735, \"Year Released\": \"2021\", \"Text Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"Video Task\": \"Video Captioning\", \"Licenses\": [{\"License\": \"Custom\", \"License URL\": \"https://docs.google.com/forms/d/e/1FAIpQLSdUOScon6uWkslgcA2-BCkAhMp86SihgKtMqX53zRf-E2fIHw/formResponse\"}], \"Creators\": [\"Massachusetts Institute of Technology\", \"IBM\", \"The University of Texas at Austin\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 416.67, \"Taken Down\": \"False\", \"Video Sources\": [\"youtube\", \"flickr\", \"vine\", \"metacafe\", \"peeks\", \"vimeo\", \"videoblocks\", \"bing videos\", \"giphy\", \"weather channel\", \"getty-images\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 0, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 0, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}, {\"Unique Dataset Identifier\": \"ego-exo4D\", \"Collection\": \"ego-exo4D\", \"Collection URL\": \"https://arxiv.org/abs/2311.18259\", \"Dataset Name\": \"Ego-Exo4D\", \"Paper Title\": \"Ego-Exo4D\", \"Paper URL\": \"https://arxiv.org/abs/2311.18259\", \"GitHub URL\": \"https://github.com/facebookresearch/Ego4d\", \"Hugging Face URL\": \"\", \"Papers with Code URL\": \"https://paperswithcode.com/paper/ego-exo4d-understanding-skilled-human/review/\", \"ArXiv URL\": \"https://arxiv.org/abs/2311.18259\", \"Semantic Scholar Corpus ID\": 265506384, \"Year Released\": \"2023\", \"Text Sources\": [\"human\"], \"Video Task\": \"Egocentric View, Exocentric view\", \"Licenses\": [{\"License\": \"MIT License\", \"License URL\": \"https://github.com/facebookresearch/Ego4d/blob/main/LICENSE\"}], \"Creators\": [\"Meta\"], \"Countries\": [\"United States of America\"], \"License Verified By\": \"Vivek Sharma\", \"Video Hours\": 1422.0, \"Taken Down\": \"False\", \"Video Sources\": [\"human\"], \"License Use (DataProvenance)\": \"commercial\", \"License Attribution (DataProvenance)\": 1, \"License Share Alike (DataProvenance)\": 0, \"License Use (DataProvenance IgnoreOpenAI)\": \"commercial\", \"License Attribution (DataProvenance IgnoreOpenAI)\": 1, \"License Share Alike (DataProvenance IgnoreOpenAI)\": 0, \"License Type\": \"Commercial\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = alt.Chart(df_speechlanguages).mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Video Task:N\",\n",
    "        title=\"Video Task\",\n",
    "        sort=None,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"count():Q\",\n",
    "        stack=\"normalize\",\n",
    "        axis=alt.Axis(format=\"%\"),\n",
    "        title=\"Pct. Datasets\"\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"License Type:N\",\n",
    "        scale=alt.Scale(\n",
    "            domain=LICENSE_ORDER,\n",
    "            range=LICENSE_PALETTE\n",
    "        ),\n",
    "        title=\"License Type\"\n",
    "    )\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=100\n",
    ")\n",
    "\n",
    "text = alt.Chart(df_speechlanguages).mark_text(\n",
    "    dy=-68,\n",
    "    align=\"center\",\n",
    "    baseline=\"top\",\n",
    "    fontSize=12\n",
    ").encode(\n",
    "    x=alt.X(\n",
    "        \"Video Task:N\",\n",
    "        title=\"Video Task to License Type\",\n",
    "        sort=None\n",
    "    ),\n",
    "    text=\"count():Q\"\n",
    ")\n",
    "\n",
    "chart = (base + text).configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    orient=LEGEND_POSITION,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ")\n",
    "\n",
    "# if PLOT_TOFILE:\n",
    "#     chart.save(\n",
    "#         os.path.join(PLOT_DIR, \"video_task-licenses.png\"),\n",
    "#         ppi=PLOT_PPI\n",
    "#     )\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(df_speechlanguages).mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Video Hours:N\",\n",
    "        title=\"Video Task\",\n",
    "        sort=None,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"count():Q\",\n",
    "        stack=\"normalize\",\n",
    "        axis=alt.Axis(format=\"%\"),\n",
    "        title=\"Pct. Datasets\"\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"License Type:N\",\n",
    "        scale=alt.Scale(\n",
    "            domain=LICENSE_ORDER,\n",
    "            range=LICENSE_PALETTE\n",
    "        ),\n",
    "        title=\"License Type\"\n",
    "    )\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=100\n",
    ")\n",
    "\n",
    "text = alt.Chart(df_speechlanguages).mark_text(\n",
    "    dy=-68,\n",
    "    align=\"center\",\n",
    "    baseline=\"top\",\n",
    "    fontSize=12\n",
    ").encode(\n",
    "    x=alt.X(\n",
    "        \"Video Task:N\",\n",
    "        title=\"Video Task to License Type\",\n",
    "        sort=None\n",
    "    ),\n",
    "    text=\"count():Q\"\n",
    ")\n",
    "\n",
    "chart = (base + text).configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    orient=LEGEND_POSITION,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ")\n",
    "\n",
    "# if PLOT_TOFILE:\n",
    "#     chart.save(\n",
    "#         os.path.join(PLOT_DIR, \"video_task-licenses.png\"),\n",
    "#         ppi=PLOT_PPI\n",
    "#     )\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: altair_viewer in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: altair in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair_viewer) (5.3.0)\n",
      "Requirement already satisfied: altair-data-server>=0.4.0 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair_viewer) (0.4.1)\n",
      "Requirement already satisfied: portpicker in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair-data-server>=0.4.0->altair_viewer) (1.6.0)\n",
      "Requirement already satisfied: tornado in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair-data-server>=0.4.0->altair_viewer) (6.4)\n",
      "Requirement already satisfied: packaging in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair->altair_viewer) (24.0)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair->altair_viewer) (2.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair->altair_viewer) (4.12.1)\n",
      "Requirement already satisfied: jinja2 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair->altair_viewer) (3.1.4)\n",
      "Requirement already satisfied: numpy in /home/gridsan/ktiwary/.local/lib/python3.10/site-packages (from altair->altair_viewer) (1.26.2)\n",
      "Requirement already satisfied: toolz in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair->altair_viewer) (0.12.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from altair->altair_viewer) (4.22.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from jsonschema>=3.0->altair->altair_viewer) (23.2.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from jsonschema>=3.0->altair->altair_viewer) (0.35.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from jsonschema>=3.0->altair->altair_viewer) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from jsonschema>=3.0->altair->altair_viewer) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from pandas>=0.25->altair->altair_viewer) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from pandas>=0.25->altair->altair_viewer) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from pandas>=0.25->altair->altair_viewer) (2024.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from jinja2->altair->altair_viewer) (2.1.5)\n",
      "Requirement already satisfied: psutil in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from portpicker->altair-data-server>=0.4.0->altair_viewer) (5.9.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/gridsan/ktiwary/.conda/envs/dpi310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.25->altair->altair_viewer) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!/home/gridsan/ktiwary/.conda/envs/dpi310/bin/pip install altair_viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_TOP_N_CATEGORIES = 10 # Number of top categories to include, rest will be grouped as \"Other\"\n",
    "\n",
    "df_sources = df_speech.explode(\"Source Category\")\n",
    "df_sources = reduce_categories_to_topk(df_sources, \"Source Category\", INCLUDE_TOP_N_CATEGORIES)\n",
    "\n",
    "sourcecategory_order = order_by_grouped_permisiveness(df_sources, \"Source Category\")\n",
    "\n",
    "df_sources[\"Source Category\"] = pd.Categorical(\n",
    "    df_sources[\"Source Category\"],\n",
    "    categories=sourcecategory_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "df_sources = df_sources.sort_values(by=\"Source Category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(\n",
    "    df_sources\n",
    ").mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Source Category:N\",\n",
    "        title=\"Source Category\",\n",
    "        axis=alt.Axis(labelAngle=-30),\n",
    "        sort=sourcecategory_order\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"count():Q\",\n",
    "        stack=\"normalize\",\n",
    "        axis=alt.Axis(format=\"%\"),\n",
    "        title=\"Pct. Datasets\"\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"License Type:N\",\n",
    "        scale=alt.Scale(\n",
    "            domain=LICENSE_ORDER,\n",
    "            range=LICENSE_PALETTE\n",
    "        ),\n",
    "        title=\"License Type\"\n",
    "    )\n",
    ").properties(\n",
    "    width=800,\n",
    "    height=100\n",
    ")\n",
    "\n",
    "text = alt.Chart(df_sources).mark_text(\n",
    "    dy=-68,\n",
    "    align=\"center\",\n",
    "    baseline=\"top\",\n",
    "    fontSize=12\n",
    ").encode(\n",
    "    x=alt.X(\n",
    "        \"Source Category:N\",\n",
    "        title=\"Source Category\",\n",
    "        sort=sourcecategory_order\n",
    "    ),\n",
    "    text=\"count():Q\"\n",
    ")\n",
    "\n",
    "chart = (base + text).configure_axis(\n",
    "        labelFontSize=FONT_SIZE,\n",
    "        titleFontSize=FONT_SIZE\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    orient=LEGEND_POSITION,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ").configure_header(\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    labelFontSize=FONT_SIZE\n",
    ")\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart.save(\n",
    "        os.path.join(PLOT_DIR, \"speech_sourcecategories-licenses.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources by Language Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_TOP_N_CATEGORIES = 6 # Number of top categories to include, rest will be grouped as \"Other\"\n",
    "\n",
    "# # Further unlist the categories of sources\n",
    "df_speechlanguagessources = reduce_categories_to_topk(\n",
    "    df_speechlanguages.explode(\"Source Category\"),\n",
    "    \"Source Category\",\n",
    "    INCLUDE_TOP_N_CATEGORIES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(\n",
    "    df_speechlanguagessources\n",
    ").mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Language Families:N\",\n",
    "        title=\"Language Family\",\n",
    "        sort=languagefamily_order,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"count():Q\",\n",
    "        stack=\"normalize\",\n",
    "        axis=alt.Axis(format=\"%\"),\n",
    "        title=\"Pct. Datasets\"\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"Source Category:N\",\n",
    "        title=\"Source Category\"\n",
    "    )\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=100\n",
    ")\n",
    "\n",
    "text = alt.Chart(df_speechlanguagessources).mark_text(\n",
    "    dy=-68,\n",
    "    align=\"center\",\n",
    "    baseline=\"top\",\n",
    "    fontSize=12\n",
    ").encode(\n",
    "    x=alt.X(\n",
    "        \"Language Families:N\",\n",
    "        title=\"Language Family\",\n",
    "        sort=languagefamily_order\n",
    "    ),\n",
    "    text=\"count():Q\"\n",
    ")\n",
    "\n",
    "chart = (base + text).configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    orient=LEGEND_POSITION,\n",
    "    columns=4,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ")\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart.save(\n",
    "        os.path.join(PLOT_DIR, \"speech_languagefamilies-sources.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hours by Language Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speechlanguageshours = df_speechlanguages.copy()\n",
    "\n",
    "# We filter out the large-scale dataset YODAS, which has a large number of hours and a large number of languages\n",
    "# Since we don't subdivide by language, it would skew results\n",
    "df_speechlanguageshours = df_speechlanguageshours[df_speechlanguageshours[\"Unique Dataset Identifier\"] != \"yodas\"]\n",
    "\n",
    "df_speechlanguageshours = df_speechlanguageshours.groupby(\"Language Families\")[\"Hours\"].sum().reset_index(name=\"Total Hours\")\n",
    "df_speechlanguageshours = df_speechlanguageshours.sort_values(by=\"Total Hours\")\n",
    "languagefamily_hourorder = df_speechlanguageshours[\"Language Families\"][::-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_speechlanguageshours).mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Language Families:N\",\n",
    "        title=\"Language Family\",\n",
    "        sort=languagefamily_hourorder,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"Total Hours:Q\",\n",
    "        # Hours of audio from datasets with each language family represented\n",
    "        # Within such datasets, we may not have specific hours for language families\n",
    "        title=\"Represented Hours\"\n",
    "    )\n",
    ").configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Coefficient Across Languages by (Cumulative) Total Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess for year labels and order\n",
    "df_speechlanguagesn = df_speechlanguages.copy()\n",
    "\n",
    "# Subdivide hours evenly across the languages given in each dataset\n",
    "df_speechlanguagesn[\"Hours\"] = df_speechlanguagesn[\"Hours\"] / df_speechlanguagesn[\"Languages\"].apply(len)\n",
    "df_speechlanguageshours = df_speechlanguagesn.explode(\"Languages\")\n",
    "\n",
    "df_speechlanguageshours = df_speechlanguageshours.sort_values(by=\"Year Released\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini coefficient for hours across languages\n",
    "speechlanguages_totalhours = df_speechlanguageshours.explode(\"Languages\").groupby(\"Languages\")[\"Hours\"].sum().reset_index(name=\"Total Hours\")\n",
    "\n",
    "gini(speechlanguages_totalhours[\"Total Hours\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cumulative hours by language over time\n",
    "df_speechlanguagescumulativehours = df_speechlanguageshours.groupby(\n",
    "    [\"Year Released\", \"Languages\"]\n",
    ")[\"Hours\"].sum().groupby(\n",
    "    \"Languages\"\n",
    ").cumsum().reset_index(name=\"Cumulative Hours\")\n",
    "\n",
    "# Calculate Gini coefficient for cumulative hours by language\n",
    "df_speechlanguagescumulativehoursgini = df_speechlanguagescumulativehours.groupby(\n",
    "    \"Year Released\"\n",
    ").apply(\n",
    "    lambda x: gini(x[\"Cumulative Hours\"].values)\n",
    ").reset_index(name=\"Gini Coefficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(\n",
    "    df_speechlanguagescumulativehoursgini\n",
    ").mark_line().encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"Gini Coefficient:Q\",\n",
    "        title=\"Gini (Cumulative)\"\n",
    "    )\n",
    ").configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=200\n",
    ")\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart.save(\n",
    "        os.path.join(PLOT_DIR, \"speech_languages-giniyears.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Category by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_TOP_N_CATEGORIES = 6\n",
    "df_speechsourceyears = df_speech.explode(\"Source Category\")\n",
    "df_speechsourceyears = reduce_categories_to_topk(df_speechsourceyears, \"Source Category\", INCLUDE_TOP_N_CATEGORIES)\n",
    "\n",
    "df_speechsourceyears = df_speechsourceyears.sort_values(by=\"Year Released\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(\n",
    "    df_speechsourceyears\n",
    ").mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"count():Q\",\n",
    "        stack=\"normalize\",\n",
    "        axis=alt.Axis(format=\"%\"),\n",
    "        title=\"Pct. Datasets\"\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"Source Category:N\",\n",
    "        title=\"Source Category\"\n",
    "    )\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=100\n",
    ")\n",
    "\n",
    "text = alt.Chart(df_speechsourceyears).mark_text(\n",
    "    dy=-68,\n",
    "    align=\"center\",\n",
    "    baseline=\"top\",\n",
    "    fontSize=12\n",
    ").encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER\n",
    "    ),\n",
    "    text=\"count():Q\"\n",
    ")\n",
    "\n",
    "chart = (base + text).configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    orient=LEGEND_POSITION,\n",
    "    columns=4,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ")\n",
    "\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart.save(\n",
    "        os.path.join(PLOT_DIR, \"speech_sourcecategories-years.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Hours by Source Category (Cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_TOP_N_CATEGORIES = 6\n",
    "\n",
    "df_speechsourceyears = df_speech.explode(\"Source Category\")\n",
    "df_speechsourceyears = reduce_categories_to_topk(df_speechsourceyears, \"Source Category\", INCLUDE_TOP_N_CATEGORIES)\n",
    "\n",
    "df_speechsourceyearscumulativehours = df_speechsourceyears.groupby(\n",
    "    [\"Year Released\", \"Source Category\"]\n",
    ")[\"Hours\"].sum().groupby(\n",
    "    \"Source Category\"\n",
    ").cumsum().reset_index(name=\"Cumulative Hours\")\n",
    "\n",
    "df_speechsourceyearscumulativehours = df_speechsourceyearscumulativehours.sort_values(by=\"Year Released\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(\n",
    "    df_speechsourceyearscumulativehours\n",
    ").mark_line().encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"Cumulative Hours:Q\",\n",
    "        title=\"Cumulative Hours\",\n",
    "        scale=alt.Scale(type=\"symlog\")\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"Source Category:N\",\n",
    "        title=\"Source Category\"\n",
    "    )\n",
    ").configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    orient=LEGEND_POSITION,\n",
    "    columns=4,\n",
    "    symbolStrokeWidth=4,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=200\n",
    ")\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart.save(\n",
    "        os.path.join(PLOT_DIR, \"speech_sourcecategories-cumulativehours.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Category (YouTube or Other) by License Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By count\n",
    "df_counts_by_license_source = df_speech.explode(\"Source\").groupby([\"License Type\", \"Source\"]).size().reset_index(name=\"Count\")\n",
    "df_counts_by_license_source = df_counts_by_license_source.sort_values(by=\"Count\")\n",
    "df_counts_by_license_source[\"YouTube\"] = df_counts_by_license_source[\"Source\"].map(\n",
    "    lambda x: \"YouTube\" if \"youtube\" in x.lower() else \"Other\"\n",
    ")\n",
    "\n",
    "# By hours\n",
    "df_hours_by_license_source = df_speech.explode(\"Source\").groupby([\"License Type\", \"Source\"])[\"Hours\"].sum().reset_index(name=\"Total Hours\")\n",
    "df_hours_by_license_source = df_hours_by_license_source.sort_values(by=\"Total Hours\")\n",
    "df_hours_by_license_source[\"YouTube\"] = df_hours_by_license_source[\"Source\"].map(\n",
    "    lambda x: \"YouTube\" if \"youtube\" in x.lower() else \"Other\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_bycount = alt.Chart(df_counts_by_license_source).mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Count:Q\",\n",
    "        title=\"Count\"\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"License Type:N\",\n",
    "        title=\"\",\n",
    "        sort=LICENSE_ORDER,\n",
    "        axis=alt.Axis(labelLimit=MAX_LABELLIMIT)\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"YouTube:N\",\n",
    "        title=\"Source\"\n",
    "    )\n",
    ").properties(\n",
    "    width=800,\n",
    "    height=100\n",
    ")\n",
    "\n",
    "chart_byhour = alt.Chart(df_hours_by_license_source).mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Total Hours:Q\",\n",
    "        title=\"Total Hours\"\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"License Type:N\",\n",
    "        title=\"License Type\",\n",
    "        sort=LICENSE_ORDER,\n",
    "        axis=alt.Axis(labelLimit=MAX_LABELLIMIT)\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"YouTube:N\",\n",
    "        title=\"Source\"\n",
    "    )\n",
    ").properties(\n",
    "    width=800,\n",
    "    height=100\n",
    ")\n",
    "\n",
    "chart = alt.vconcat(chart_bycount, chart_byhour).configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    orient=LEGEND_POSITION,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ")\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart.save(\n",
    "        os.path.join(PLOT_DIR, \"speech_license-source.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creator Categories by Year\n",
    "\n",
    "Note: we use the original annotations here instead of the DPI constants, for a different view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speechcategoriesyears = df_speech.explode(\"Creator Categories\")\n",
    "df_speechcategoriesyears = df_speechcategoriesyears.sort_values(by=\"Year Released\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(\n",
    "    df_speechcategoriesyears\n",
    ").mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER,\n",
    "        axis=alt.Axis(labelAngle=-30)\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        \"count():Q\",\n",
    "        stack=\"normalize\",\n",
    "        axis=alt.Axis(format=\"%\"),\n",
    "        title=\"Pct. Datasets\"\n",
    "    ),\n",
    "    color=alt.Color(\n",
    "        \"Creator Categories:N\",\n",
    "        title=\"Creator Category\"\n",
    "    )\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=100\n",
    ")\n",
    "\n",
    "text = alt.Chart(df_speechsourceyears).mark_text(\n",
    "    dy=-68,\n",
    "    align=\"center\",\n",
    "    baseline=\"top\",\n",
    "    fontSize=12\n",
    ").encode(\n",
    "    x=alt.X(\n",
    "        \"Year Released:N\",\n",
    "        title=\"Year Released\",\n",
    "        sort=YEARS_ORDER\n",
    "    ),\n",
    "    text=\"count():Q\"\n",
    ")\n",
    "\n",
    "chart = (base + text).configure_axis(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE\n",
    ").configure_legend(\n",
    "    labelFontSize=FONT_SIZE,\n",
    "    titleFontSize=FONT_SIZE,\n",
    "    orient=LEGEND_POSITION,\n",
    "    columns=4,\n",
    "    labelLimit=MAX_LABELLIMIT\n",
    ")\n",
    "\n",
    "if PLOT_TOFILE:\n",
    "    chart.save(\n",
    "        os.path.join(PLOT_DIR, \"speech_categories-years.png\"),\n",
    "        ppi=PLOT_PPI\n",
    "    )\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of License Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licensetype_counts = df_speech[\"License Type\"].value_counts()\n",
    "df_licensetypes = pd.concat([\n",
    "    licensetype_counts,\n",
    "    (licensetype_counts / licensetype_counts.sum()).round(4) * 100\n",
    "], axis=1)\n",
    "\n",
    "df_licensetypes.columns = [\"Count\", \"Pct.\"]\n",
    "\n",
    "df_licensetypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables of YouTube Dataset LicenseTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By count\n",
    "df_speech[\"YouTube\"] = df_speech[\"Source\"].map(\n",
    "    lambda x: \"YouTube\" if any(\"youtube\" in xi.lower() for xi in x) else \"Other\"\n",
    ")\n",
    "\n",
    "df_youtube = df_speech.groupby([\"License Type\", \"YouTube\"]).size().reset_index(name=\"Count\")\n",
    "df_youtube = df_youtube.sort_values(by=\"Count\")\n",
    "df_youtube[\"Pct.\"] = df_youtube.groupby(\"License Type\")[\"Count\"].transform(lambda x: (x / x.sum()).round(4) * 100)\n",
    "\n",
    "df_youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By hours\n",
    "df_youtubehours = df_speech.groupby([\"License Type\", \"YouTube\"])[\"Hours\"].sum().reset_index(name=\"Total Hours\")\n",
    "df_youtubehours = df_youtubehours.sort_values(by=\"Total Hours\")\n",
    "df_youtubehours[\"Pct.\"] = df_youtubehours.groupby(\"License Type\")[\"Total Hours\"].transform(lambda x: (x / x.sum()).round(4) * 100)\n",
    "\n",
    "df_youtubehours"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpiv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
